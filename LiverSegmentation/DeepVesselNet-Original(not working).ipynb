{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUWfUwsnO0tj"
      },
      "source": [
        "# Imports\n",
        "\n",
        "Import all required libraries in this cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eVFEHDbNqfg",
        "outputId": "0b390c76-08ce-4144-d78f-33f962e96f8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import niwidgets as nw\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from itertools import product\n",
        "from collections import OrderedDict\n",
        "from sklearn.metrics import f1_score, precision_recall_curve\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchio as tio\n",
        "\n",
        "# set up default cuda device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('device:', device)\n",
        "\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcD8DWysPArP"
      },
      "source": [
        "Define necessary variables related to train dataset and val dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUuFvbOhNQ8A",
        "outputId": "22fc4061-a96d-4b7f-a524-dfb210a3ad18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training_set_path: dataset\\train\n",
            "validation_set_path: dataset\\val\n",
            "training_images_path: dataset\\train\\images\n",
            "training_labels_path: dataset\\train\\labels\n",
            "training_images: ['liver1.nii', 'liver10.nii', 'liver11.nii', 'liver12.nii', 'liver13.nii', 'liver14.nii', 'liver15.nii', 'liver16.nii', 'liver17.nii', 'liver18.nii', 'liver2.nii', 'liver3.nii', 'liver4.nii', 'liver5.nii', 'liver6.nii', 'liver7.nii', 'liver8.nii', 'liver9.nii']\n",
            "training_labels: ['liver1.nii', 'liver10.nii', 'liver11.nii', 'liver12.nii', 'liver13.nii', 'liver14.nii', 'liver15.nii', 'liver16.nii', 'liver17.nii', 'liver18.nii', 'liver2.nii', 'liver3.nii', 'liver4.nii', 'liver5.nii', 'liver6.nii', 'liver7.nii', 'liver8.nii', 'liver9.nii']\n",
            "validation_images_path: dataset\\val\\images\n",
            "validation_labels_path: dataset\\val\\labels\n",
            "validation_images: ['liver19.nii', 'liver20.nii']\n",
            "validation_labels: ['liver19.nii', 'liver20.nii']\n"
          ]
        }
      ],
      "source": [
        "dataset_path = 'dataset'\n",
        "training_set_path = os.path.join(dataset_path, 'train')\n",
        "print('training_set_path:', training_set_path)\n",
        "validation_set_path = os.path.join(dataset_path, 'val')\n",
        "print('validation_set_path:', validation_set_path)\n",
        "\n",
        "training_images_path = os.path.join(training_set_path, 'images')\n",
        "print('training_images_path:', training_images_path)\n",
        "training_labels_path = os.path.join(training_set_path, 'labels')\n",
        "print('training_labels_path:', training_labels_path)\n",
        "training_images = os.listdir(training_images_path)\n",
        "print('training_images:', training_images)\n",
        "training_labels = os.listdir(training_labels_path)\n",
        "print('training_labels:', training_labels)\n",
        "\n",
        "validation_images_path = os.path.join(validation_set_path, 'images')\n",
        "print('validation_images_path:', validation_images_path)\n",
        "validation_labels_path = os.path.join(validation_set_path, 'labels')\n",
        "print('validation_labels_path:', validation_labels_path)\n",
        "validation_images = os.listdir(validation_images_path)\n",
        "print('validation_images:', validation_images)\n",
        "validation_labels = os.listdir(validation_labels_path)\n",
        "print('validation_labels:', validation_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx39x7_5PsMY"
      },
      "source": [
        "# Define Custom Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55q8RmVAIamk"
      },
      "source": [
        "## SyntheticData2 (custom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TBpZysKxIamy"
      },
      "outputs": [],
      "source": [
        "class SyntheticData2(data.Dataset):\n",
        "    \"\"\" \n",
        "    Class defined to handle the synthetic dataset\n",
        "    derived from pytorch's Dataset class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root_path, patch_size=64, mode='train'):\n",
        "        self.TAG = '[SyntheticData2]'\n",
        "        self.root_dir_name = root_path\n",
        "        self.raw_dir_name = os.path.join(self.root_dir_name, 'images')\n",
        "        self.seg_dir_name = os.path.join(self.root_dir_name, 'labels')\n",
        "        self.mode = mode\n",
        "        self.patch_size = patch_size\n",
        "        print(self.TAG, '[raw_dir_name]', self.raw_dir_name)\n",
        "        print(self.TAG, '[seg_dir_name]', self.seg_dir_name)\n",
        "        \n",
        "        # Finding first and last file names in directory\n",
        "        self.file_names = os.listdir(self.raw_dir_name)\n",
        "        self.file_names.sort(key=lambda name: int(name[5:-4]))\n",
        "\n",
        "        self.transforms = tio.Compose([\n",
        "            # tio.OneOf({\n",
        "            #     tio.RandomAffine(): 0.5,\n",
        "            #     tio.RandomElasticDeformation(): 0.5,\n",
        "            # }, p=0.5),\n",
        "            # tio.RandomFlip(axes=(0, 1, 2)),\n",
        "            tio.ZNormalization(),\n",
        "        ])\n",
        "\n",
        "        self.patches_raw = []\n",
        "        self.patches_seg = []\n",
        "        for image_name in self.file_names:\n",
        "            raw, seg = self.get_patches_from_nii(image_name)\n",
        "            if self.mode == 'test':\n",
        "                # print(f'[{self.mode}]', image_name, raw.shape, seg.shape)\n",
        "                raw, seg = raw.unsqueeze(0), seg.unsqueeze(0)\n",
        "            self.patches_raw += raw\n",
        "            self.patches_seg += seg\n",
        "        \n",
        "        print(self.TAG, '[patches_raw]', len(self.patches_raw), self.patches_raw[0].shape)\n",
        "        print(self.TAG, '[patches_seg]', len(self.patches_seg), self.patches_seg[0].shape)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "            return [self[i] for i in range(index)]\n",
        "        elif isinstance(index, slice):\n",
        "            return [self[i] for i in range(*index.indices(len(self)))]\n",
        "        elif isinstance(index, int):\n",
        "            raw = self.patches_raw[index]\n",
        "            seg = self.patches_seg[index]\n",
        "            if self.mode == 'test':\n",
        "                # print(self.TAG, '[__getitem__]', raw.shape, seg.shape)\n",
        "                raw = raw.unsqueeze(0)\n",
        "            raw = self.transforms(raw)\n",
        "            return raw, seg\n",
        "        else:\n",
        "            raise TypeError('Invalid argument type.')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patches_raw)\n",
        "\n",
        "    def get_image_path(self, index):\n",
        "        return (os.path.join(self.raw_dir_name, self.file_names[index]), \n",
        "                os.path.join(self.seg_dir_name, self.file_names[index]))\n",
        "\n",
        "    def get_patches_from_nii(self, image_name):\n",
        "        raw_img_name = os.path.join(self.raw_dir_name, image_name)\n",
        "        seg_img_name = os.path.join(self.seg_dir_name, image_name)\n",
        "        \n",
        "        # Load proxy so image not loaded into memory\n",
        "        raw_proxy = nib.load(raw_img_name)\n",
        "        seg_proxy = nib.load(seg_img_name)\n",
        "\n",
        "        # Get dataobj of proxy\n",
        "        raw_data = np.asarray(raw_proxy.dataobj).astype(np.int32)\n",
        "        seg_data = np.asarray(seg_proxy.dataobj).astype(np.int32)\n",
        "        \n",
        "        if self.mode == 'test':\n",
        "            return torch.from_numpy(raw_data), torch.from_numpy(seg_data)\n",
        "        \n",
        "        # This is where the patch is defined\n",
        "        raw_patch = torch.from_numpy(raw_data). \\\n",
        "            unfold(2, self.patch_size, self.patch_size). \\\n",
        "            unfold(1, self.patch_size, self.patch_size). \\\n",
        "            unfold(0, self.patch_size, self.patch_size)\n",
        "        raw_patch = raw_patch.contiguous(). \\\n",
        "            view(-1, 1, self.patch_size, self.patch_size, self.patch_size)\n",
        "\n",
        "        seg_patch = torch.from_numpy(seg_data). \\\n",
        "            unfold(2, self.patch_size, self.patch_size). \\\n",
        "            unfold(1, self.patch_size, self.patch_size). \\\n",
        "            unfold(0, self.patch_size, self.patch_size)\n",
        "        seg_patch = seg_patch.contiguous(). \\\n",
        "            view(-1, self.patch_size, self.patch_size, self.patch_size)\n",
        "        \n",
        "        seg_patch_area = seg_patch.sum(axis=[1, 2, 3])\n",
        "        useful_patches = np.where(seg_patch_area > 0)[0]\n",
        "\n",
        "        return raw_patch[useful_patches], seg_patch[useful_patches]\n",
        "        # return raw_patch, seg_patch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYZXuaNtQAlH"
      },
      "source": [
        "## Create train/val dataset objects using SyntheticData class and train/val dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKyiJxTPTkwj",
        "outputId": "582c81fe-efac-4e69-f70a-77e0384cc84c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SyntheticData2] [raw_dir_name] dataset\\train\\images\n",
            "[SyntheticData2] [seg_dir_name] dataset\\train\\labels\n",
            "[SyntheticData2] [patches_raw] 269 torch.Size([1, 64, 64, 64])\n",
            "[SyntheticData2] [patches_seg] 269 torch.Size([64, 64, 64])\n",
            "[SyntheticData2] [raw_dir_name] dataset\\val\\images\n",
            "[SyntheticData2] [seg_dir_name] dataset\\val\\labels\n",
            "[SyntheticData2] [patches_raw] 37 torch.Size([1, 64, 64, 64])\n",
            "[SyntheticData2] [patches_seg] 37 torch.Size([64, 64, 64])\n",
            "Train size: 269\n",
            "Validation size: 37\n",
            "Img size: torch.Size([1, 64, 64, 64]) torch.float32\n",
            "Segmentation size: torch.Size([64, 64, 64]) torch.int32\n",
            "train_loader (total batches): 135\n",
            "val_loader (total batches): 19\n"
          ]
        }
      ],
      "source": [
        "patch_size = 64\n",
        "train_synthetic = SyntheticData2(root_path=training_set_path, patch_size=patch_size)\n",
        "# print(len(train_synthetic[0]), train_synthetic[0][0].shape, train_synthetic[0][1].shape)\n",
        "val_synthetic = SyntheticData2(root_path=validation_set_path, patch_size=patch_size)\n",
        "# print(len(val_synthetic[0]), val_synthetic[0][0].shape, val_synthetic[0][1].shape)\n",
        "\n",
        "print(\"Train size:\", len(train_synthetic))\n",
        "print(\"Validation size:\", len(val_synthetic))\n",
        "print(\"Img size:\", train_synthetic[0][0].size(), train_synthetic[0][0].dtype)\n",
        "print(\"Segmentation size:\", train_synthetic[0][1].size(), train_synthetic[0][1].dtype)\n",
        "\n",
        "train_loader = data.DataLoader(train_synthetic, batch_size=2, shuffle=True, num_workers=0)\n",
        "val_loader = data.DataLoader(val_synthetic, batch_size=2, shuffle=False, num_workers=0)\n",
        "print('train_loader (total batches):', len(train_loader))\n",
        "print('val_loader (total batches):', len(val_loader))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tacPY5Hfm_k"
      },
      "source": [
        "# Define Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96Ky6INLd53X"
      },
      "source": [
        "## Conv3d_CrossHair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OS0iNYPNeBMg"
      },
      "outputs": [],
      "source": [
        "class Conv3d_CrossHair(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "        self.convD = nn.Conv3d(in_channels, out_channels, (1, kernel_size, kernel_size), stride, (0, padding, padding))\n",
        "        self.convH = nn.Conv3d(in_channels, out_channels, (kernel_size, 1, kernel_size), stride, (padding, 0, padding))\n",
        "        self.convW = nn.Conv3d(in_channels, out_channels, (kernel_size, kernel_size, 1), stride, (padding, padding, 0))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        outD = self.convD(x)\n",
        "        outH = self.convH(x)\n",
        "        outW = self.convW(x)\n",
        "        # print('[outD]', outD.shape)\n",
        "        # print('[outH]', outH.shape)\n",
        "        # print('[outW]', outW.shape)\n",
        "        out = outD + outH + outW\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DeepVesselNetFCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DeepVesselNetFCN(nn.Module):\n",
        "    def __init__(self, nchannels=1, nlabels=2, dim=3, batchnorm=True, dropout=False):\n",
        "        super().__init__()\n",
        "        self.nchannels = nchannels\n",
        "        self.nlabels = nlabels\n",
        "        self.dims = dim\n",
        "        self.batchnorm = batchnorm\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        # 1st layer\n",
        "        self.conv1 = Conv3d_CrossHair(in_channels=self.nchannels, out_channels=5, kernel_size=3, padding=1)\n",
        "        self.batchnorm1 = nn.BatchNorm3d(5)\n",
        "        self.dropout1 = nn.Dropout(p=0.25)\n",
        "        # 2nd layer\n",
        "        self.conv2 = Conv3d_CrossHair(in_channels=5, out_channels=10, kernel_size=5, padding=2)\n",
        "        self.batchnorm2 = nn.BatchNorm3d(10)\n",
        "        self.dropout2 = nn.Dropout(p=0.25)\n",
        "        # 3rd layer\n",
        "        self.conv3 = Conv3d_CrossHair(in_channels=10, out_channels=20, kernel_size=5, padding=2)\n",
        "        self.batchnorm3 = nn.BatchNorm3d(20)\n",
        "        self.dropout3 = nn.Dropout(p=0.25)\n",
        "        # 4th layer\n",
        "        self.conv4 = Conv3d_CrossHair(in_channels=20, out_channels=50, kernel_size=3, padding=1)\n",
        "        self.batchnorm4 = nn.BatchNorm3d(50)\n",
        "        self.dropout4 = nn.Dropout(p=0.25)\n",
        "        # fully convolutional layer\n",
        "        self.fcn1 = nn.Conv3d(in_channels=50, out_channels=self.nlabels, kernel_size=1)\n",
        "        # Softmax layer\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        # non linearities\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # initialize weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1st layer\n",
        "        x = self.conv1(x)\n",
        "        x = self.batchnorm1(x) if self.batchnorm else x\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout1(x) if self.dropout else x\n",
        "        # 2nd layer\n",
        "        x = self.conv2(x)\n",
        "        x = self.batchnorm2(x) if self.batchnorm else x\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout2(x) if self.dropout else x\n",
        "        # 3rd layer\n",
        "        x = self.conv3(x)\n",
        "        x = self.batchnorm3(x) if self.batchnorm else x\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout3(x) if self.dropout else x\n",
        "        # 4th layer\n",
        "        x = self.conv4(x)\n",
        "        x = self.batchnorm4(x) if self.batchnorm else x\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout4(x) if self.dropout else x\n",
        "        # 5th layer\n",
        "        x = self.fcn1(x)\n",
        "        x = self.sigmoid(x)\n",
        "        # classification layer\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "    def save(self, path):\n",
        "        print('Saving model... %s' % path)\n",
        "        torch.save(self, path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbjFph5SdCAa"
      },
      "source": [
        "## DeepVesselNetFCN2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_GCZs880e6qe"
      },
      "outputs": [],
      "source": [
        "class DeepVesselNetFCN2(nn.Module):\n",
        "    def __init__(self, nchannels=1, nlabels=2, dim=3, batchnorm=True, dropout=False):\n",
        "        super().__init__()\n",
        "        self.nchannels = nchannels\n",
        "        self.nlabels = nlabels\n",
        "        self.dims = dim\n",
        "        self.batchnorm = batchnorm\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # 1st layer\n",
        "        self.conv1 = Conv3d_CrossHair(in_channels=self.nchannels, out_channels=5, kernel_size=3, padding=1)\n",
        "        self.batchnorm1 = nn.BatchNorm3d(5)\n",
        "        self.dropout1 = nn.Dropout(p=0.25)\n",
        "        # 2nd layer\n",
        "        self.conv2_1 = Conv3d_CrossHair(in_channels=5, out_channels=5, kernel_size=3, padding=1)\n",
        "        self.batchnorm2_1 = nn.BatchNorm3d(5)\n",
        "        self.conv2_2 = Conv3d_CrossHair(in_channels=5, out_channels=10, kernel_size=3, padding=1)\n",
        "        self.batchnorm2_2 = nn.BatchNorm3d(10)\n",
        "        self.dropout2 = nn.Dropout(p=0.25)\n",
        "        # 3rd layer\n",
        "        self.conv3_1 = Conv3d_CrossHair(in_channels=10, out_channels=10, kernel_size=3, padding=1)\n",
        "        self.batchnorm3_1 = nn.BatchNorm3d(10)\n",
        "        self.conv3_2 = Conv3d_CrossHair(in_channels=10, out_channels=20, kernel_size=3, padding=1)\n",
        "        self.batchnorm3_2 = nn.BatchNorm3d(20)\n",
        "        self.dropout3 = nn.Dropout(p=0.25)\n",
        "        # 4th layer\n",
        "        self.conv4 = Conv3d_CrossHair(in_channels=20, out_channels=50, kernel_size=3, padding=1)\n",
        "        self.batchnorm4 = nn.BatchNorm3d(50)\n",
        "        self.dropout4 = nn.Dropout(p=0.25)\n",
        "        # fully convolutional layer\n",
        "        self.fcn1 = nn.Conv3d(in_channels=50, out_channels=self.nlabels, kernel_size=1)\n",
        "        # softmax layer\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        # non linearities\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "        # initialize weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1st layer\n",
        "        x = self.conv1(x)\n",
        "        x = self.batchnorm1(x) if self.batchnorm else x\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout1(x) if self.dropout else x\n",
        "        # 2nd layer\n",
        "        x = self.conv2_1(x)\n",
        "        x = self.batchnorm2_1(x) if self.batchnorm else x\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout2(x) if self.dropout else x\n",
        "        x = self.conv2_2(x)\n",
        "        x = self.batchnorm2_2(x) if self.batchnorm else x\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout2(x) if self.dropout else x\n",
        "        # 3rd layer\n",
        "        x = self.conv3_1(x)\n",
        "        x = self.batchnorm3_1(x) if self.batchnorm else x\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout3(x) if self.dropout else x\n",
        "        x = self.conv3_2(x)\n",
        "        x = self.batchnorm3_2(x) if self.batchnorm else x\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout3(x) if self.dropout else x\n",
        "        # 4rd layer\n",
        "        x = self.conv4(x)\n",
        "        x = self.batchnorm4(x) if self.batchnorm else x\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout4(x) if self.dropout else x\n",
        "        # 5th layer\n",
        "        x = self.fcn1(x)\n",
        "        x = self.sigmoid(x)\n",
        "        # classification layer\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "    def save(self, path):\n",
        "        print('Saving model... %s' % path)\n",
        "        torch.save(self, path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9uXQNvlX4CU"
      },
      "source": [
        "## DeepVesselNet-UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "W_XnQsObX7xL"
      },
      "outputs": [],
      "source": [
        "class DeepVesselNet_UNet(nn.Module):\n",
        "    def __init__(self, nchannels=1, nlabels=2, dim=3, init_features=32, debug=False):\n",
        "        super(DeepVesselNet_UNet, self).__init__()\n",
        "        self.TAG = '[DeepVesselNet_UNet]'\n",
        "        self.nchannels = nchannels\n",
        "        self.nlabels = nlabels\n",
        "        self.dims = dim\n",
        "        self.init_features = init_features\n",
        "        self.debug = debug\n",
        "        \n",
        "        # encoder layers\n",
        "        self.encoder1 = DeepVesselNet_UNet._block(nchannels, init_features, name='enc1')\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        self.encoder2 = DeepVesselNet_UNet._block(init_features, init_features * 2, name='enc2')\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        self.encoder3 = DeepVesselNet_UNet._block(init_features * 2, init_features * 4, name='enc3')\n",
        "        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        self.encoder4 = DeepVesselNet_UNet._block(init_features * 4, init_features * 8, name='enc4')\n",
        "        self.pool4 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        # bottleneck layer\n",
        "        self.bottleneck = DeepVesselNet_UNet._block(init_features * 8, init_features * 16, name='bottleneck')\n",
        "        # decoder layers\n",
        "        self.upconv4 = nn.ConvTranspose3d(init_features * 16, init_features * 8, kernel_size=2, stride=2)\n",
        "        self.decoder4 = DeepVesselNet_UNet._block((init_features * 8) * 2, init_features * 8, name='dec4')\n",
        "        self.upconv3 = nn.ConvTranspose3d(init_features * 8, init_features * 4, kernel_size=2, stride=2)\n",
        "        self.decoder3 = DeepVesselNet_UNet._block((init_features * 4) * 2, init_features * 4, name='dec3')\n",
        "        self.upconv2 = nn.ConvTranspose3d(init_features * 4, init_features * 2, kernel_size=2, stride=2)\n",
        "        self.decoder2 = DeepVesselNet_UNet._block((init_features * 2) * 2, init_features * 2, name='dec2')\n",
        "        self.upconv1 = nn.ConvTranspose3d(init_features * 2, init_features, kernel_size=2, stride=2)\n",
        "        self.decoder1 = DeepVesselNet_UNet._block(init_features * 2, init_features, name='dec1')\n",
        "        # output layer\n",
        "        self.conv = nn.Conv3d(in_channels=init_features, out_channels=nlabels, kernel_size=1)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.debug: print(self.TAG, '[x]', x.shape)\n",
        "        enc1 = self.encoder1(x)\n",
        "        if self.debug: print(self.TAG, '[enc1]', enc1.shape)\n",
        "        enc2 = self.encoder2(self.pool1(enc1))\n",
        "        if self.debug: print(self.TAG, '[enc2]', enc2.shape)\n",
        "        enc3 = self.encoder3(self.pool2(enc2))\n",
        "        if self.debug: print(self.TAG, '[enc3]', enc3.shape)\n",
        "        enc4 = self.encoder4(self.pool3(enc3))\n",
        "        if self.debug: print(self.TAG, '[enc4]', enc4.shape)\n",
        "\n",
        "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
        "        if self.debug: print(self.TAG, '[bottleneck]', bottleneck.shape)\n",
        "\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "        if self.debug: print(self.TAG, '[upconv4]', dec4.shape)\n",
        "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
        "        if self.debug: print(self.TAG, '[cat4]', dec4.shape)\n",
        "        dec4 = self.decoder4(dec4)\n",
        "        if self.debug: print(self.TAG, '[dec4]', dec4.shape)\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        if self.debug: print(self.TAG, '[upconv3]', dec3.shape)\n",
        "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
        "        if self.debug: print(self.TAG, '[cat3]', dec3.shape)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "        if self.debug: print(self.TAG, '[dec3]', dec3.shape)\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        if self.debug: print(self.TAG, '[upconv2]', dec2.shape)\n",
        "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
        "        if self.debug: print(self.TAG, '[cat2]', dec2.shape)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "        if self.debug: print(self.TAG, '[dec2]', dec2.shape)\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        if self.debug: print(self.TAG, '[upconv1]', dec1.shape)\n",
        "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
        "        if self.debug: print(self.TAG, '[cat1]', dec1.shape)\n",
        "        dec1 = self.decoder1(dec1)\n",
        "        if self.debug: print(self.TAG, '[dec1]', dec1.shape)\n",
        "        return self.softmax(self.sigmoid(self.conv(dec1)))\n",
        "\n",
        "    @staticmethod\n",
        "    def _block(in_channels, features, name):\n",
        "        return nn.Sequential(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    (f'{name}_conv1', Conv3d_CrossHair(in_channels=in_channels, out_channels=features, kernel_size=3, padding=1)),\n",
        "                    (f'{name}_norm1', nn.BatchNorm3d(num_features=features)),\n",
        "                    (f'{name}_relu1', nn.ReLU(inplace=True)),\n",
        "                    (f'{name}_conv2', Conv3d_CrossHair(in_channels=features, out_channels=features, kernel_size=3, padding=1)),\n",
        "                    (f'{name}_norm2', nn.BatchNorm3d(num_features=features)),\n",
        "                    (f'{name}_relu2', nn.ReLU(inplace=True)),\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def save(self, path):\n",
        "        print('Saving model... %s' % path)\n",
        "        torch.save(self, path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RX0TIwD9HGh"
      },
      "source": [
        "## DeepVesselNet-VNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LQHO26lY9Jtm"
      },
      "outputs": [],
      "source": [
        "def passthrough(x, **kwargs):\n",
        "    return x\n",
        "\n",
        "def ELUCons(elu, nchan):\n",
        "    if elu:\n",
        "        return nn.ELU(inplace=True)\n",
        "    else:\n",
        "        return nn.PReLU(nchan)\n",
        "\n",
        "# normalization between sub-volumes is necessary for good performance\n",
        "class ContBatchNorm3d(nn.modules.batchnorm._BatchNorm):\n",
        "    def _check_input_dim(self, input):\n",
        "        if input.dim() != 5:\n",
        "            raise ValueError('expected 5D input (got {}D input)'.format(input.dim()))\n",
        "        # super(ContBatchNorm3d, self)._check_input_dim(input)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self._check_input_dim(input)\n",
        "        return F.batch_norm(input, self.running_mean, self.running_var, self.weight, self.bias, True, self.momentum, self.eps)\n",
        "\n",
        "\n",
        "class LUConv(nn.Module):\n",
        "    def __init__(self, nchan, elu):\n",
        "        super(LUConv, self).__init__()\n",
        "        self.conv1 = Conv3d_CrossHair(nchan, nchan, kernel_size=5, padding=2)\n",
        "        self.bn1 = ContBatchNorm3d(nchan)\n",
        "        self.relu1 = ELUCons(elu, nchan)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu1(self.bn1(self.conv1(x)))\n",
        "        return out\n",
        "\n",
        "\n",
        "def _make_nConv(nchan, depth, elu):\n",
        "    layers = []\n",
        "    for _ in range(depth):\n",
        "        layers.append(LUConv(nchan, elu))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class InputTransition(nn.Module):\n",
        "    def __init__(self, outChans, elu, debug=False):\n",
        "        super(InputTransition, self).__init__()\n",
        "        self.TAG = '[InputTransition]'\n",
        "        self.debug = debug\n",
        "        self.conv1 = Conv3d_CrossHair(1, 16, kernel_size=5, padding=2)\n",
        "        self.bn1 = ContBatchNorm3d(16)\n",
        "        self.relu1 = ELUCons(elu, 16)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # do we want a PRELU here as well?\n",
        "        out = self.bn1(self.conv1(x))\n",
        "        if self.debug: print(self.TAG, '[out]', out.shape)\n",
        "        # split input in to 16 channels\n",
        "        x16 = torch.cat((x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x), 1)\n",
        "        if self.debug: print(self.TAG, '[x16]', x16.shape)\n",
        "        out = self.relu1(torch.add(out, x16))\n",
        "        return out\n",
        "\n",
        "\n",
        "class DownTransition(nn.Module):\n",
        "    def __init__(self, inChans, nConvs, elu, dropout=False):\n",
        "        super(DownTransition, self).__init__()\n",
        "        outChans = 2 * inChans\n",
        "        self.down_conv = Conv3d_CrossHair(inChans, outChans, kernel_size=2, stride=2)\n",
        "        self.bn1 = ContBatchNorm3d(outChans)\n",
        "        self.do1 = passthrough\n",
        "        self.relu1 = ELUCons(elu, outChans)\n",
        "        self.relu2 = ELUCons(elu, outChans)\n",
        "        if dropout:\n",
        "            self.do1 = nn.Dropout3d()\n",
        "        self.ops = _make_nConv(outChans, nConvs, elu)\n",
        "\n",
        "    def forward(self, x):\n",
        "        down = self.relu1(self.bn1(self.down_conv(x)))\n",
        "        out = self.do1(down)\n",
        "        out = self.ops(out)\n",
        "        out = self.relu2(torch.add(out, down))\n",
        "        return out\n",
        "\n",
        "\n",
        "class UpTransition(nn.Module):\n",
        "    def __init__(self, inChans, outChans, nConvs, elu, dropout=False):\n",
        "        super(UpTransition, self).__init__()\n",
        "        self.up_conv = nn.ConvTranspose3d(inChans, outChans // 2, kernel_size=2, stride=2)\n",
        "        self.bn1 = ContBatchNorm3d(outChans // 2)\n",
        "        self.do1 = passthrough\n",
        "        self.do2 = nn.Dropout3d()\n",
        "        self.relu1 = ELUCons(elu, outChans // 2)\n",
        "        self.relu2 = ELUCons(elu, outChans)\n",
        "        if dropout:\n",
        "            self.do1 = nn.Dropout3d()\n",
        "        self.ops = _make_nConv(outChans, nConvs, elu)\n",
        "\n",
        "    def forward(self, x, skipx):\n",
        "        out = self.do1(x)\n",
        "        skipxdo = self.do2(skipx)\n",
        "        out = self.relu1(self.bn1(self.up_conv(out)))\n",
        "        xcat = torch.cat((out, skipxdo), 1)\n",
        "        out = self.ops(xcat)\n",
        "        out = self.relu2(torch.add(out, xcat))\n",
        "        return out\n",
        "\n",
        "\n",
        "class OutputTransition(nn.Module):\n",
        "    def __init__(self, inChans, elu, nll):\n",
        "        super(OutputTransition, self).__init__()\n",
        "        self.conv1 = Conv3d_CrossHair(inChans, 2, kernel_size=5, padding=2)\n",
        "        self.bn1 = ContBatchNorm3d(2)\n",
        "        self.relu1 = ELUCons(elu, 2)\n",
        "        self.conv2 = nn.Conv3d(2, 2, kernel_size=1)\n",
        "        # if nll:\n",
        "        #     self.softmax = F.log_softmax\n",
        "        # else:\n",
        "        #     self.softmax = F.softmax\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # convolve 32 down to 2 channels\n",
        "        out = self.relu1(self.bn1(self.conv1(x)))\n",
        "        out = self.conv2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        out = self.softmax(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DeepVesselNet_VNet(nn.Module):\n",
        "    # the number of convolutions in each layer corresponds\n",
        "    # to what is in the actual prototxt, not the intent\n",
        "    def __init__(self, elu=True, nll=False, debug=False):\n",
        "        super(DeepVesselNet_VNet, self).__init__()\n",
        "        self.TAG = '[DeepVesselNet_VNet]'\n",
        "        self.debug = debug\n",
        "        self.in_tr = InputTransition(16, elu, debug=debug)\n",
        "        self.down_tr32 = DownTransition(16, 1, elu)\n",
        "        self.down_tr64 = DownTransition(32, 2, elu)\n",
        "        self.down_tr128 = DownTransition(64, 3, elu, dropout=True)\n",
        "        self.down_tr256 = DownTransition(128, 2, elu, dropout=True)\n",
        "        self.up_tr256 = UpTransition(256, 256, 2, elu, dropout=True)\n",
        "        self.up_tr128 = UpTransition(256, 128, 2, elu, dropout=True)\n",
        "        self.up_tr64 = UpTransition(128, 64, 1, elu)\n",
        "        self.up_tr32 = UpTransition(64, 32, 1, elu)\n",
        "        self.out_tr = OutputTransition(32, elu, nll)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.debug: print(self.TAG, '[x]', x.shape)\n",
        "        out16 = self.in_tr(x)\n",
        "        if self.debug: print(self.TAG, '[out16]', out16.shape)\n",
        "        out32 = self.down_tr32(out16)\n",
        "        if self.debug: print(self.TAG, '[out32]', out32.shape)\n",
        "        out64 = self.down_tr64(out32)\n",
        "        if self.debug: print(self.TAG, '[out64]', out64.shape)\n",
        "        out128 = self.down_tr128(out64)\n",
        "        if self.debug: print(self.TAG, '[out128]', out128.shape)\n",
        "        out256 = self.down_tr256(out128)\n",
        "        if self.debug: print(self.TAG, '[out256]', out256.shape)\n",
        "        out = self.up_tr256(out256, out128)\n",
        "        if self.debug: print(self.TAG, '[out][1]', out.shape)\n",
        "        out = self.up_tr128(out, out64)\n",
        "        if self.debug: print(self.TAG, '[out][2]', out.shape)\n",
        "        out = self.up_tr64(out, out32)\n",
        "        if self.debug: print(self.TAG, '[out][3]', out.shape)\n",
        "        out = self.up_tr32(out, out16)\n",
        "        if self.debug: print(self.TAG, '[out][4]', out.shape)\n",
        "        out = self.out_tr(out)\n",
        "        if self.debug: print(self.TAG, '[out][5]', out.shape)\n",
        "        return out\n",
        "\n",
        "    def save(self, path):\n",
        "        print('Saving model... %s' % path)\n",
        "        torch.save(self, path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87Btl15iPxFF"
      },
      "source": [
        "# Define Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "guceNOWvlf1n"
      },
      "outputs": [],
      "source": [
        "def _categorical_crossentropy(target, output, from_logits=False, dim=-1, debug=False):\n",
        "    TAG = '[_categorical_crossentropy]'\n",
        "    output_dimensions = list(range(len(output.shape)))\n",
        "    if dim != -1 and dim not in output_dimensions:\n",
        "        raise ValueError(\n",
        "            '{}{}{}'.format(\n",
        "                'Unexpected channels dim {}. '.format(dim),\n",
        "                'Expected to be -1 or one of the axes of `output`, ',\n",
        "                'which has {} dimensions.'.format(len(output.get_shape()))))\n",
        "    # Note: tf.nn.softmax_cross_entropy_with_logits\n",
        "    # expects logits, Keras expects probabilities.\n",
        "    if not from_logits:\n",
        "        # scale preds so that the class probas of each sample sum to 1\n",
        "        if debug: print(TAG, '[output]', output.shape, output.min(), output.max())\n",
        "        output = output / torch.sum(output, dim, True)\n",
        "        if debug: print(TAG, '[output]', output.shape, output.min(), output.max())\n",
        "        # manual computation of crossentropy\n",
        "        _epsilon = torch.tensor(1e-7, dtype=output.dtype).to(device)\n",
        "        output = torch.clamp(output, _epsilon, 1. - _epsilon)\n",
        "        if debug: print(TAG, '[output]', output.shape, output.min(), output.max())\n",
        "        return target * torch.log(output)\n",
        "    # else:\n",
        "    #     return tf.nn.softmax_cross_entropy_with_logits(labels=target,logits=output)\n",
        "\n",
        "def weighted_categorical_crossentropy_with_fpr(dim=1, from_logits=False, classes=2, threshold=0.5, debug=False):\n",
        "    def loss(y_pred, y_true):\n",
        "        TAG = '[loss]'\n",
        "        if debug: print(TAG, '[y_pred]', y_pred.shape, y_pred.min(), y_pred.max())\n",
        "        y_true = F.one_hot(y_true.long(), num_classes=2).permute(0, 4, 1, 2, 3)\n",
        "        if debug: print(TAG, '[y_true]', y_true.shape, y_true.min(), y_true.max())\n",
        "        L = _categorical_crossentropy(target=y_true, output=y_pred, dim=dim, from_logits=from_logits, debug=debug)\n",
        "        if debug: print(TAG, '[L]', L.shape, L.min(), L.max(), L.sum())\n",
        "        y_true_p = torch.argmax(y_true, dim=dim)\n",
        "        if debug: print(TAG, '[y_true_p]', y_true_p.shape, y_true_p.min(), y_true_p.max(), y_true_p.dtype)\n",
        "        y_pred_bin = (y_pred >= threshold).to(y_true.dtype) if from_logits else torch.argmax(y_pred, dim=dim)\n",
        "        if debug: print(TAG, '[y_pred_bin]', y_pred_bin.shape, y_pred_bin.min(), y_pred_bin.max(), y_pred_bin.dtype)\n",
        "        y_pred_probs = y_pred if from_logits else torch.max(y_pred, dim=dim)[0]\n",
        "        if debug: print(TAG, '[y_pred_probs]', y_pred_probs.shape, y_pred_probs.min(), y_pred_probs.max(), y_pred_probs.dtype)\n",
        "        _epsilon = 1e-7\n",
        "        pred_dtype = y_pred.dtype\n",
        "        # select indices where ground truth voxels are background\n",
        "        y_true_neg = (y_true_p == 0).to(pred_dtype)\n",
        "        if debug: print(TAG, '[y_true_neg]', y_true_neg.shape, y_true_neg.min(), y_true_neg.max(), y_true_neg.sum())\n",
        "        # select indices where ground truth voxels are vessel\n",
        "        y_true_pos = (y_true_p == 1).to(pred_dtype)\n",
        "        if debug: print(TAG, '[y_true_pos]', y_true_pos.shape, y_true_pos.min(), y_true_pos.max(), y_true_pos.sum())\n",
        "        # calculate loss weight for background class\n",
        "        wa_neg = 1. / (torch.sum(y_true_neg) + _epsilon)\n",
        "        if debug: print(TAG, '[wa_neg]', wa_neg)\n",
        "        # calculate loss weight for vessel class\n",
        "        wa_pos = 1. / (torch.sum(y_true_pos) + _epsilon)\n",
        "        if debug: print(TAG, '[wa_pos]', wa_pos)\n",
        "        # calculate L1 (as given in paper)\n",
        "        L1 = torch.sum(wa_neg * L * y_true_neg) + torch.sum(wa_pos * L * y_true_pos)\n",
        "        if debug: print(TAG, '[L1]', L1)\n",
        "        # select indices where ground truth voxels are vessel but model predicted background (false negative)\n",
        "        false_neg = (y_true_p != 0).to(pred_dtype) * (y_pred_bin == 0).to(pred_dtype)\n",
        "        if debug: print(TAG, '[false_neg]', false_neg.shape, false_neg.min(), false_neg.max(), false_neg.sum())\n",
        "        # select indices where ground truth voxels are background but model predicted vessel (false positive)\n",
        "        false_pos = (y_true_p != 1).to(pred_dtype) * (y_pred_bin == 1).to(pred_dtype)\n",
        "        if debug: print(TAG, '[false_pos]', false_pos.shape, false_pos.min(), false_pos.max(), false_pos.sum())\n",
        "        # calculate gamma for false negative predictions (as given in paper)\n",
        "        gamma_neg = 0.5 + (torch.sum(torch.abs((false_neg * y_pred_probs) - 0.5)) / (torch.sum(false_neg) + _epsilon))\n",
        "        if debug: print(TAG, '[gamma_neg]', gamma_neg)\n",
        "        # calculate gamma for false positive predictions (as given in paper)\n",
        "        gamma_pos = 0.5 + (torch.sum(torch.abs((false_pos * y_pred_probs) - 0.5)) / (torch.sum(false_pos) + _epsilon))\n",
        "        if debug: print(TAG, '[gamma_pos]', gamma_pos)\n",
        "        # calculate loss weight for false negative predictions\n",
        "        wb_neg = wa_neg * gamma_neg\n",
        "        if debug: print(TAG, '[wb_neg]', wb_neg)\n",
        "        # calculate loss weight for false positive predictions\n",
        "        wb_pos = wa_pos * gamma_pos\n",
        "        if debug: print(TAG, '[wb_pos]', wb_pos)\n",
        "        # calculate L2 (as given in paper)\n",
        "        L2 = torch.sum(wb_neg * L * false_neg) + torch.sum(wb_pos * L * false_pos)\n",
        "        if debug: print(TAG, '[L2]', L2)\n",
        "        # calculate total loss (as given in paper)\n",
        "        total_loss = L1 + L2\n",
        "        if debug: print(TAG, '[total_loss]', total_loss)\n",
        "        return total_loss\n",
        "    return loss\n",
        "\n",
        "def weighted_categorical_crossentropy_with_fpr_2(dim=1, from_logits=False, classes=2, threshold=0.5, debug=False):\n",
        "    def loss(y_pred, y_true):\n",
        "        TAG = '[loss]'\n",
        "        if debug: print(TAG, '[y_pred]', y_pred.shape, y_pred.min(), y_pred.max())\n",
        "        y_true = F.one_hot(y_true.long(), num_classes=2).permute(0, 4, 1, 2, 3)\n",
        "        if debug: print(TAG, '[y_true]', y_true.shape, y_true.min(), y_true.max())\n",
        "        L = _categorical_crossentropy(target=y_true, output=y_pred, dim=dim, from_logits=from_logits)\n",
        "        if debug: print(TAG, '[L]', L.shape, L.min(), L.max(), L.sum())\n",
        "        y_true_p = torch.argmax(y_true, dim=dim)\n",
        "        if debug: print(TAG, '[y_true_p]', y_true_p.shape, y_true_p.min(), y_true_p.max(), y_true_p.dtype)\n",
        "        y_pred_bin = (y_pred >= threshold).to(y_true.dtype) if from_logits else torch.argmax(y_pred, dim=dim)\n",
        "        if debug: print(TAG, '[y_pred_bin]', y_pred_bin.shape, y_pred_bin.min(), y_pred_bin.max(), y_pred_bin.dtype)\n",
        "        y_pred_probs = y_pred if from_logits else torch.max(y_pred, dim=dim)[0]\n",
        "        if debug: print(TAG, '[y_pred_probs]', y_pred_probs.shape, y_pred_probs.min(), y_pred_probs.max(), y_pred_probs.dtype)\n",
        "        _epsilon = 1e-7\n",
        "        C = 0\n",
        "        for c in range(classes):\n",
        "            c_true = (y_true_p == c).to(y_pred.dtype)\n",
        "            if debug: print(TAG, c, '[c_true]', c_true.shape, c_true.dtype, c_true.min(), c_true.max())\n",
        "            w = 1. / (torch.sum(c_true) + _epsilon)\n",
        "            C += torch.sum(L * c_true * w)\n",
        "            # Calc. FP Rate Correction\n",
        "            c_false_p = (y_true_p != c).to(y_pred.dtype) * (y_pred_bin == c).to(y_pred.dtype) # Calculate false predictions\n",
        "            if debug: print(TAG, c, '[c_false_p]', c_false_p.shape, c_false_p.dtype, c_false_p.min(), c_false_p.max())\n",
        "            gamma = 0.5 + (torch.sum(torch.abs((c_false_p * y_pred_probs) - 0.5)) / (torch.sum(c_false_p) + _epsilon)) # Calculate Gamme\n",
        "            wc = w * gamma # gamma / |Y+|\n",
        "            C += torch.sum(L * c_false_p * wc) # Add FP Correction\n",
        "        return C\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ex0SLv23NW-K"
      },
      "outputs": [],
      "source": [
        "def weighted_categorical_crossentropy_with_fpr(dim=1, from_logits=False, classes=2, threshold=0.5, debug=False):\n",
        "    def loss(y_pred, y_true):\n",
        "        TAG = '[loss]'\n",
        "        _epsilon = 1e-7\n",
        "        true_dtype = y_true.dtype\n",
        "        dim_123 = (1, 2, 3)\n",
        "\n",
        "        if debug: print(TAG, '[y_pred]', y_pred.shape, y_pred.min(), y_pred.max())\n",
        "        y_true = F.one_hot(y_true.long(), num_classes=2).permute(0, 4, 1, 2, 3)\n",
        "        if debug: print(TAG, '[y_true]', y_true.shape, y_true.min(), y_true.max())\n",
        "\n",
        "        # true binary volume\n",
        "        y_true_p = torch.argmax(y_true, dim=dim)\n",
        "        if debug: print(TAG, '[y_true_p]', y_true_p.shape, y_true_p.min(), y_true_p.max(), y_true_p.dtype)\n",
        "        # predicted binary volume\n",
        "        y_pred_bin = (y_pred >= threshold).to(true_dtype) if from_logits else torch.argmax(y_pred, dim=dim)\n",
        "        if debug: print(TAG, '[y_pred_bin]', y_pred_bin.shape, y_pred_bin.min(), y_pred_bin.max(), y_pred_bin.dtype)\n",
        "\n",
        "        # select indices where ground truth voxels are background\n",
        "        y_true_pos = (y_true_p == 1).to(true_dtype)\n",
        "        if debug: print(TAG, '[y_true_pos]', y_true_pos.shape, y_true_pos.min(), y_true_pos.max(), y_true_pos.sum(dim=dim_123))\n",
        "        y_true_neg = (y_true_p == 0).to(true_dtype)\n",
        "        if debug: print(TAG, '[y_true_neg]', y_true_neg.shape, y_true_neg.min(), y_true_neg.max(), y_true_neg.sum(dim=dim_123))\n",
        "        # select indices where ground truth voxels are vessel\n",
        "\n",
        "        y_prob_pos = y_pred[:, 1]\n",
        "        if debug: print(TAG, '[y_prob_pos]', y_prob_pos.shape, y_prob_pos.min(), y_prob_pos.max(), y_prob_pos.dtype)\n",
        "        y_prob_neg = y_pred[:, 0]\n",
        "        if debug: print(TAG, '[y_prob_neg]', y_prob_neg.shape, y_prob_neg.min(), y_prob_neg.max(), y_prob_neg.dtype)\n",
        "\n",
        "        log_prob = - y_true * torch.log(y_pred)\n",
        "        if debug: print(TAG, '[log_prob]', log_prob.shape, log_prob.min(), log_prob.max(), log_prob.sum(dim=(2, 3, 4)))\n",
        "        # if debug: print(TAG, '[log_prob[y_true_neg]]', log_prob[0, 0][y_true_neg[0] == 1].sum(), log_prob[0, 0].sum())\n",
        "        # if debug: print(TAG, '[log_prob[y_true_pos]]', log_prob[0, 1][y_true_pos[0] == 1].sum(), log_prob[0, 1].sum())\n",
        "\n",
        "        # calculate loss weight for vessel class\n",
        "        wa_pos = 1. / (torch.sum(y_true_pos, dim=dim_123) + _epsilon)\n",
        "        if debug: print(TAG, '[wa_pos]', wa_pos)\n",
        "        # calculate loss weight for background class\n",
        "        wa_neg = 1. / (torch.sum(y_true_neg, dim=dim_123) + _epsilon)\n",
        "        if debug: print(TAG, '[wa_neg]', wa_neg)\n",
        "\n",
        "        # calculate L1 (as given in paper)\n",
        "        L1 = wa_pos * torch.sum(log_prob[:, 1]) + wa_neg * torch.sum(log_prob[:, 0])\n",
        "        if debug: print(TAG, '[L1]', L1)\n",
        "\n",
        "        # select indices where ground truth voxels are background but model predicted vessel (false positive)\n",
        "        false_pos = (y_true_p == 0).to(true_dtype) * (y_pred_bin == 1).to(true_dtype)\n",
        "        if debug: print(TAG, '[false_pos]', false_pos.shape, false_pos.min(), false_pos.max(), false_pos.sum(dim=dim_123))\n",
        "        # select indices where ground truth voxels are vessel but model predicted background (false negative)\n",
        "        false_neg = (y_true_p == 1).to(true_dtype) * (y_pred_bin == 0).to(true_dtype)\n",
        "        if debug: print(TAG, '[false_neg]', false_neg.shape, false_neg.min(), false_neg.max(), false_neg.sum(dim=dim_123))\n",
        "\n",
        "        # calculate gamma for false positive predictions (as given in paper)\n",
        "        gamma_pos = 0.5 + (torch.sum(torch.abs((false_pos * y_prob_neg) - 0.5), dim=dim_123) / (torch.sum(false_pos, dim=dim_123) + _epsilon))\n",
        "        if debug: print(TAG, '[gamma_pos]', gamma_pos)\n",
        "        # calculate gamma for false negative predictions (as given in paper)\n",
        "        gamma_neg = 0.5 + (torch.sum(torch.abs((false_neg * y_prob_pos) - 0.5), dim=dim_123) / (torch.sum(false_neg, dim=dim_123) + _epsilon))\n",
        "        if debug: print(TAG, '[gamma_neg]', gamma_neg)\n",
        "\n",
        "        # calculate loss weight for false positive predictions\n",
        "        wb_pos = wa_pos * gamma_pos\n",
        "        if debug: print(TAG, '[wb_pos]', wb_pos)\n",
        "        # calculate loss weight for false negative predictions\n",
        "        wb_neg = wa_neg * gamma_neg\n",
        "        if debug: print(TAG, '[wb_neg]', wb_neg)\n",
        "\n",
        "        # calculate L2 (as given in paper)\n",
        "        L2 = wb_pos * torch.sum(log_prob[:, 0] * false_pos) + wb_neg * torch.sum(log_prob[:, 1] * false_neg)\n",
        "        if debug: print(TAG, '[L2]', L2)\n",
        "\n",
        "        # calculate total loss (as given in paper)\n",
        "        total_loss = L1 + L2\n",
        "        if debug: print(TAG, '[total_loss]', total_loss)\n",
        "\n",
        "        total_mean_loss = torch.mean(total_loss)\n",
        "        if debug: print(TAG, '[total_mean_loss]', total_mean_loss)\n",
        "\n",
        "        return total_mean_loss\n",
        "\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMKsOPhVUvop"
      },
      "source": [
        "# Define Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "85EeOYcpUwhO"
      },
      "outputs": [],
      "source": [
        "def dice_score(y_true, y_pred):\n",
        "    return f1_score(y_true.flatten(), y_pred.flatten())\n",
        "\n",
        "def dice_information(y_true, y_pred):\n",
        "    prec, rec, thres = precision_recall_curve(y_true.flatten(), y_pred.flatten())\n",
        "    f1 = (2. * prec * rec) / (prec + rec)\n",
        "    ind = np.argmax(f1)\n",
        "    return prec[ind], rec[ind], f1[ind], thres[ind]\n",
        "\n",
        "def threshold_accuracy(y_true, y_pred, threshold=0.5):\n",
        "    pred = (y_pred >= threshold).astype(np.int32)\n",
        "    return np.mean(y_true.astype(np.int32) == pred)\n",
        "\n",
        "def categorical_accuracy(y_true, y_pred, axis=-1):\n",
        "    return np.mean(np.argmax(y_true, axis=axis) == np.argmax(y_pred, axis=axis))\n",
        "\n",
        "def dice(y_true, y_pred, smooth=1):\n",
        "    intersection = np.sum(y_true * y_pred, axis=list(range(1, len(y_true.shape))))\n",
        "    union = np.sum(y_true, axis=list(range(1, len(y_true.shape)))) + np.sum(y_pred, axis=list(range(1, len(y_true.shape))))\n",
        "    return np.mean((2. * intersection + smooth) / (union + smooth), axis=0)\n",
        "\n",
        "def dice_coeff(outputs, targets, smooth=1, pred=False, debug=False):\n",
        "    if debug: print('[dice_coeff]', '[outputs, targets]', outputs.shape, targets.shape)\n",
        "    if pred:\n",
        "        if debug: print('if')\n",
        "        pred = outputs\n",
        "    else:\n",
        "        if debug: print('else')\n",
        "        _, pred = torch.max(outputs, 1)\n",
        "        if debug: print('[dice_coeff]', '[pred]', np.unique(pred.detach().cpu().numpy()))\n",
        "    \n",
        "    if debug: print('[dice_coeff]', '[pred, targets]', pred.shape, targets.shape)\n",
        "    pred = F.one_hot(pred.long(), num_classes=2)\n",
        "    targets = F.one_hot(targets.long(), num_classes=2)\n",
        "    if debug: print('[dice_coeff]', '[pred, targets]', pred.shape, targets.shape)\n",
        "\n",
        "    dim = tuple(range(1, len(pred.shape) - 1))\n",
        "    if debug: print('[dice_coeff]', '[dim]', dim)\n",
        "    intersection = torch.sum(targets * pred, dim=dim, dtype=torch.float)\n",
        "    if debug: print('[dice_coeff]', '[intersection]', intersection)\n",
        "    union = torch.sum(targets, dim=dim, dtype=torch.float) + torch.sum(pred, dim=dim, dtype=torch.float)\n",
        "    if debug: print('[dice_coeff]', '[union]', union)\n",
        "    if debug: print('[dice_coeff]', '[intersection, union]', torch.mean(intersection), torch.mean(union))\n",
        "    dice = torch.mean((2. * intersection + smooth) / (union + smooth), dtype=torch.float)\n",
        "    if debug: print('[dice_coeff]', '[dice]', dice)\n",
        "    return dice\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTKLxDs3ZZxn"
      },
      "source": [
        "# Define Solver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "caTcoEj4ZauX"
      },
      "outputs": [],
      "source": [
        "class Solver(object):\n",
        "\n",
        "    default_optim_args = {'lr': 0.01, 'weight_decay': 0.}\n",
        "\n",
        "    def __init__(self, optim=torch.optim.SGD, optim_args={}, loss_func=weighted_categorical_crossentropy_with_fpr()):\n",
        "        self.TAG = '[Solver]'\n",
        "        print(self.TAG)\n",
        "        optim_args_merged = self.default_optim_args.copy()\n",
        "        optim_args_merged.update(optim_args)\n",
        "        self.optim_args = optim_args_merged\n",
        "        self.optim = optim\n",
        "        self.loss_func = loss_func\n",
        "        self.best_train_dice = -1\n",
        "        self.best_val_dice = -1\n",
        "        self.best_train_model = None\n",
        "        self.best_val_model = None\n",
        "\n",
        "        self._reset_histories()\n",
        "        self.writer = SummaryWriter()\n",
        "\n",
        "    def _reset_histories(self):\n",
        "        \"\"\"Resets train and val histories for the accuracy and the loss. \"\"\"\n",
        "        self.train_loss_history = []\n",
        "        self.train_dice_history = []\n",
        "        self.train_accu_history = []\n",
        "        self.val_loss_history = []\n",
        "        self.val_dice_history = []\n",
        "        self.val_accu_history = []\n",
        "    \n",
        "    def cpu_np(self, tensor):\n",
        "        return tensor.detach().cpu().numpy()\n",
        "\n",
        "    def train(self, model, train_loader, val_loader, num_epochs=10, log_nth=0, model_save_path=''):\n",
        "        \"\"\"\n",
        "        Train a given model with the provided data.\n",
        "        Inputs:\n",
        "        - model: object initialized from a torch.nn.Module\n",
        "        - train_loader: train data (currently using nonsense data)\n",
        "        - val_loader: val data (currently using nonsense data)\n",
        "        - num_epochs: total number of epochs\n",
        "        - log_nth: log training accuracy and loss every nth iteration\n",
        "        \"\"\"\n",
        "\n",
        "        optim = self.optim(model.parameters(), **self.optim_args)\n",
        "        lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.9)\n",
        "        self._reset_histories()\n",
        "        iter_per_epoch = len(train_loader)\n",
        "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        model.to(device)\n",
        "\n",
        "        print('START TRAIN')\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            ##########################\n",
        "            ######## TRAINING ########\n",
        "            ##########################\n",
        "            model.train()\n",
        "            train_loss_scores = []\n",
        "            train_dice_scores = []\n",
        "            train_accu_scores = []\n",
        "\n",
        "            for i, (inputs, targets) in enumerate(train_loader, 1):\n",
        "                # if i > 24: break\n",
        "                inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.long)\n",
        "                \n",
        "                optim.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = self.loss_func(outputs, targets)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "\n",
        "                preds = torch.argmax(outputs, 1)\n",
        "                self.train_loss_history.append(self.cpu_np(loss))\n",
        "                train_loss_scores.append(self.cpu_np(loss))\n",
        "                train_dice_scores.append(self.cpu_np(dice_coeff(outputs, targets)))\n",
        "                train_accu_scores.append(np.mean(self.cpu_np(preds == targets)))\n",
        "\n",
        "                if log_nth and i % log_nth == 0:\n",
        "                    train_loss_mean = np.mean(train_loss_scores[-log_nth:])\n",
        "                    train_dice_mean = np.mean(train_dice_scores[-log_nth:])\n",
        "                    train_accu_mean = np.mean(train_accu_scores[-log_nth:])\n",
        "                    log_string = f'[TRAIN][Iter][{i + epoch * iter_per_epoch}/{iter_per_epoch * num_epochs}]'\n",
        "                    log_string += f' loss: {train_loss_mean:.4f}'\n",
        "                    log_string += f', dice: {train_dice_mean:.4f}'\n",
        "                    log_string += f', accu: {train_accu_mean:.4f}'\n",
        "                    print(log_string)\n",
        "                    self.writer.add_scalar('Loss/Train', train_loss_mean, i + epoch * iter_per_epoch)\n",
        "                    self.writer.add_scalar('Dice/Train', train_dice_mean, i + epoch * iter_per_epoch)\n",
        "                    self.writer.add_scalar('Accuracy/Train', train_accu_mean, i + epoch * iter_per_epoch)\n",
        "            \n",
        "            self.writer.add_scalar('Learning-Rate', lr_scheduler.get_lr()[0], epoch + 1)\n",
        "            lr_scheduler.step()\n",
        "            train_loss_epoch = np.mean(train_loss_scores)\n",
        "            train_dice_epoch = np.mean(train_dice_scores)\n",
        "            train_accu_epoch = np.mean(train_accu_scores)\n",
        "            self.train_dice_history.append(train_dice_epoch)\n",
        "            self.train_accu_history.append(train_accu_epoch)\n",
        "\n",
        "            if log_nth:\n",
        "                log_string = f'[TRAIN][Epoch][{epoch + 1}/{num_epochs}]'\n",
        "                log_string += f' loss: {train_loss_epoch:.4f}'\n",
        "                log_string += f', dice: {train_dice_epoch:.4f}'\n",
        "                log_string += f', accu: {train_accu_epoch:.4f}'\n",
        "                print(log_string)\n",
        "                model.save(model_save_path)\n",
        "\n",
        "            ##########################\n",
        "            ####### VALIDATION #######\n",
        "            ##########################\n",
        "            model.eval()\n",
        "            val_loss_scores = []\n",
        "            val_dice_scores = []\n",
        "            val_accu_scores = []\n",
        "\n",
        "            for i, (inputs, targets) in enumerate(val_loader, 1):\n",
        "                # if i > 24: break\n",
        "                inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.long)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = self.loss_func(outputs, targets)\n",
        "\n",
        "                preds = torch.argmax(outputs, 1)\n",
        "                self.train_loss_history.append(self.cpu_np(loss))\n",
        "                val_loss_scores.append(self.cpu_np(loss))\n",
        "                val_dice_scores.append(self.cpu_np(dice_coeff(outputs, targets)))\n",
        "                val_accu_scores.append(np.mean(self.cpu_np(preds == targets)))\n",
        "\n",
        "            val_loss_epoch = np.mean(val_loss_scores)\n",
        "            val_dice_epoch = np.mean(val_dice_scores)\n",
        "            val_accu_epoch = np.mean(val_accu_scores)\n",
        "            self.val_dice_history.append(val_dice_epoch)\n",
        "            self.val_accu_history.append(val_accu_epoch)\n",
        "            \n",
        "            if log_nth:\n",
        "                log_string = f'[VAL][Epoch][{epoch + 1}/{num_epochs}]'\n",
        "                log_string += f' loss: {val_loss_epoch:.4f}'\n",
        "                log_string += f', dice: {val_dice_epoch:.4f}'\n",
        "                log_string += f', accu: {val_accu_epoch:.4f}'\n",
        "                print(log_string)\n",
        "                self.writer.add_scalar('Loss/Val', val_loss_epoch, epoch + 1)\n",
        "                self.writer.add_scalar('Dice/Val', val_dice_epoch, epoch + 1)\n",
        "                self.writer.add_scalar('Accuracy/Val', val_accu_epoch, epoch + 1)\n",
        "\n",
        "        print(\"FINISH\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYawVISVPIHl"
      },
      "source": [
        "# Train Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWo7LjlXRfmT"
      },
      "source": [
        "Create model object and view it's architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Av7roR9RfD5",
        "outputId": "c4a4a1f7-97d2-4a65-b64d-236465b853b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DeepVesselNetFCN(\n",
            "  (conv1): Conv3d_CrossHair(\n",
            "    (convD): Conv3d(1, 5, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
            "    (convH): Conv3d(1, 5, kernel_size=(3, 1, 3), stride=(1, 1, 1), padding=(1, 0, 1))\n",
            "    (convW): Conv3d(1, 5, kernel_size=(3, 3, 1), stride=(1, 1, 1), padding=(1, 1, 0))\n",
            "  )\n",
            "  (batchnorm1): BatchNorm3d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout1): Dropout(p=0.25, inplace=False)\n",
            "  (conv2): Conv3d_CrossHair(\n",
            "    (convD): Conv3d(5, 10, kernel_size=(1, 5, 5), stride=(1, 1, 1), padding=(0, 2, 2))\n",
            "    (convH): Conv3d(5, 10, kernel_size=(5, 1, 5), stride=(1, 1, 1), padding=(2, 0, 2))\n",
            "    (convW): Conv3d(5, 10, kernel_size=(5, 5, 1), stride=(1, 1, 1), padding=(2, 2, 0))\n",
            "  )\n",
            "  (batchnorm2): BatchNorm3d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout2): Dropout(p=0.25, inplace=False)\n",
            "  (conv3): Conv3d_CrossHair(\n",
            "    (convD): Conv3d(10, 20, kernel_size=(1, 5, 5), stride=(1, 1, 1), padding=(0, 2, 2))\n",
            "    (convH): Conv3d(10, 20, kernel_size=(5, 1, 5), stride=(1, 1, 1), padding=(2, 0, 2))\n",
            "    (convW): Conv3d(10, 20, kernel_size=(5, 5, 1), stride=(1, 1, 1), padding=(2, 2, 0))\n",
            "  )\n",
            "  (batchnorm3): BatchNorm3d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout3): Dropout(p=0.25, inplace=False)\n",
            "  (conv4): Conv3d_CrossHair(\n",
            "    (convD): Conv3d(20, 50, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
            "    (convH): Conv3d(20, 50, kernel_size=(3, 1, 3), stride=(1, 1, 1), padding=(1, 0, 1))\n",
            "    (convW): Conv3d(20, 50, kernel_size=(3, 3, 1), stride=(1, 1, 1), padding=(1, 1, 0))\n",
            "  )\n",
            "  (batchnorm4): BatchNorm3d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout4): Dropout(p=0.25, inplace=False)\n",
            "  (fcn1): Conv3d(50, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
            "  (softmax): Softmax(dim=1)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = DeepVesselNetFCN(batchnorm=True, dropout=True).to(device)\n",
        "# model = DeepVesselNetFCN2(batchnorm=True, dropout=True).to(device)\n",
        "# model = DeepVesselNet_UNet().to(device)\n",
        "# model = DeepVesselNet_VNet().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_wCIlPISP84"
      },
      "source": [
        "Define optimizer parameters and Solver object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSAJC8utSOwo",
        "outputId": "712e067c-fb1c-4cdb-dc37-b07586a932e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "optim_args: {'lr': 0.01, 'weight_decay': 0.005, 'momentum': 0.9, 'nesterov': True}\n",
            "[Solver]\n"
          ]
        }
      ],
      "source": [
        "model_save_path = 'models/deepvesselnet-fcn-01.model'\n",
        "total_epochs = 20\n",
        "optim_args = {'lr': 0.01, 'weight_decay': 0.005, 'momentum': 0.9, 'nesterov': True}\n",
        "# optim_args = {'lr': 1e-3, 'weight_decay': 0.01}\n",
        "optim = torch.optim.SGD\n",
        "# optim = torch.optim.Adam\n",
        "print('optim_args:', optim_args)\n",
        "solver = Solver(optim_args=optim_args, optim=optim, loss_func=weighted_categorical_crossentropy_with_fpr())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv3XTzuiSSzl"
      },
      "source": [
        "Start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ao3xwCm3OH9E",
        "outputId": "2afe48d5-1923-49e1-e432-a6eb8feda8cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "START TRAIN\n",
            "[TRAIN][Iter][5/2700] loss: 653.2379, dice: 0.4326, accu: 0.8318\n",
            "[TRAIN][Iter][10/2700] loss: 66.5997, dice: 0.5006, accu: 0.9880\n",
            "[TRAIN][Iter][15/2700] loss: 872.5706, dice: 0.5314, accu: 0.9846\n",
            "[TRAIN][Iter][20/2700] loss: 18.8451, dice: 0.4953, accu: 0.9793\n",
            "[TRAIN][Iter][25/2700] loss: 59.5376, dice: 0.5057, accu: 0.9925\n",
            "[TRAIN][Iter][30/2700] loss: 30.5637, dice: 0.4965, accu: 0.9839\n",
            "[TRAIN][Iter][35/2700] loss: 266.0258, dice: 0.4977, accu: 0.9759\n",
            "[TRAIN][Iter][40/2700] loss: 481.1839, dice: 0.5139, accu: 0.9821\n",
            "[TRAIN][Iter][45/2700] loss: 107.8097, dice: 0.4974, accu: 0.9774\n",
            "[TRAIN][Iter][50/2700] loss: 71.4449, dice: 0.5048, accu: 0.9943\n",
            "[TRAIN][Iter][55/2700] loss: 145.8036, dice: 0.4972, accu: 0.9718\n",
            "[TRAIN][Iter][60/2700] loss: 216.8252, dice: 0.5154, accu: 0.9917\n",
            "[TRAIN][Iter][65/2700] loss: 7.4229, dice: 0.4963, accu: 0.9790\n",
            "[TRAIN][Iter][70/2700] loss: 44.4985, dice: 0.5024, accu: 0.9935\n",
            "[TRAIN][Iter][75/2700] loss: 24.3509, dice: 0.4974, accu: 0.9841\n",
            "[TRAIN][Iter][80/2700] loss: 205.6769, dice: 0.5058, accu: 0.9928\n",
            "[TRAIN][Iter][85/2700] loss: 12.8001, dice: 0.5007, accu: 0.9952\n",
            "[TRAIN][Iter][90/2700] loss: 13.7378, dice: 0.5014, accu: 0.9869\n",
            "[TRAIN][Iter][95/2700] loss: 16.9264, dice: 0.4997, accu: 0.9875\n",
            "[TRAIN][Iter][100/2700] loss: 180.0881, dice: 0.5224, accu: 0.9819\n",
            "[TRAIN][Iter][105/2700] loss: 119.4473, dice: 0.5035, accu: 0.9887\n",
            "[TRAIN][Iter][110/2700] loss: 12.8068, dice: 0.4976, accu: 0.9880\n",
            "[TRAIN][Iter][115/2700] loss: 24.6902, dice: 0.4947, accu: 0.9771\n",
            "[TRAIN][Iter][120/2700] loss: 9.6593, dice: 0.4973, accu: 0.9861\n",
            "[TRAIN][Iter][125/2700] loss: 1143.0354, dice: 0.5199, accu: 0.9902\n",
            "[TRAIN][Iter][130/2700] loss: 9.8110, dice: 0.5050, accu: 0.9959\n",
            "[TRAIN][Iter][135/2700] loss: 8.9184, dice: 0.4921, accu: 0.9634\n",
            "[TRAIN][Epoch][1/20] loss: 178.6784, dice: 0.5009, accu: 0.9794\n",
            "Saving model... models/deepvesselnet-fcn-01.model\n",
            "[VAL][Epoch][1/20] loss: 22.9215, dice: 0.4993, accu: 0.9874\n",
            "[TRAIN][Iter][140/2700] loss: 140.5603, dice: 0.5127, accu: 0.9778\n",
            "[TRAIN][Iter][145/2700] loss: 24.9489, dice: 0.5059, accu: 0.9839\n",
            "[TRAIN][Iter][150/2700] loss: 137.7878, dice: 0.5027, accu: 0.9909\n",
            "[TRAIN][Iter][155/2700] loss: 15.8452, dice: 0.4964, accu: 0.9820\n",
            "[TRAIN][Iter][160/2700] loss: 8.2403, dice: 0.4995, accu: 0.9927\n",
            "[TRAIN][Iter][165/2700] loss: 19.5919, dice: 0.5026, accu: 0.9913\n",
            "[TRAIN][Iter][170/2700] loss: 269.2470, dice: 0.5054, accu: 0.9782\n",
            "[TRAIN][Iter][175/2700] loss: 14.5013, dice: 0.4994, accu: 0.9928\n",
            "[TRAIN][Iter][180/2700] loss: 58.6309, dice: 0.4925, accu: 0.9659\n",
            "[TRAIN][Iter][185/2700] loss: 5.4168, dice: 0.4986, accu: 0.9907\n",
            "[TRAIN][Iter][190/2700] loss: 28.7252, dice: 0.5016, accu: 0.9836\n",
            "[TRAIN][Iter][195/2700] loss: 16.5274, dice: 0.5017, accu: 0.9941\n",
            "[TRAIN][Iter][200/2700] loss: 58.7736, dice: 0.5023, accu: 0.9902\n",
            "[TRAIN][Iter][205/2700] loss: 37.8589, dice: 0.5021, accu: 0.9904\n",
            "[TRAIN][Iter][210/2700] loss: 684.7756, dice: 0.5230, accu: 0.9748\n",
            "[TRAIN][Iter][215/2700] loss: 27.1785, dice: 0.4962, accu: 0.9828\n",
            "[TRAIN][Iter][220/2700] loss: 31.7514, dice: 0.5060, accu: 0.9816\n",
            "[TRAIN][Iter][225/2700] loss: 478.1197, dice: 0.5061, accu: 0.9836\n",
            "[TRAIN][Iter][230/2700] loss: 11.5195, dice: 0.4996, accu: 0.9903\n",
            "[TRAIN][Iter][235/2700] loss: 207.1460, dice: 0.5006, accu: 0.9847\n",
            "[TRAIN][Iter][240/2700] loss: 210.5892, dice: 0.5121, accu: 0.9794\n",
            "[TRAIN][Iter][245/2700] loss: 21.4261, dice: 0.4997, accu: 0.9927\n",
            "[TRAIN][Iter][250/2700] loss: 57.7552, dice: 0.5008, accu: 0.9882\n",
            "[TRAIN][Iter][255/2700] loss: 11.9532, dice: 0.4985, accu: 0.9911\n",
            "[TRAIN][Iter][260/2700] loss: 271.7073, dice: 0.5328, accu: 0.9866\n",
            "[TRAIN][Iter][265/2700] loss: 58.6323, dice: 0.5017, accu: 0.9934\n",
            "[TRAIN][Iter][270/2700] loss: 14.4615, dice: 0.4967, accu: 0.9807\n",
            "[TRAIN][Epoch][2/20] loss: 108.2841, dice: 0.5036, accu: 0.9857\n",
            "Saving model... models/deepvesselnet-fcn-01.model\n",
            "[VAL][Epoch][2/20] loss: 22.9215, dice: 0.4993, accu: 0.9874\n",
            "[TRAIN][Iter][275/2700] loss: 28.2188, dice: 0.4984, accu: 0.9884\n",
            "[TRAIN][Iter][280/2700] loss: 47.8158, dice: 0.5026, accu: 0.9939\n",
            "[TRAIN][Iter][285/2700] loss: 10.8405, dice: 0.4971, accu: 0.9845\n",
            "[TRAIN][Iter][290/2700] loss: 16.0817, dice: 0.4967, accu: 0.9845\n",
            "[TRAIN][Iter][295/2700] loss: 6.1418, dice: 0.4940, accu: 0.9743\n",
            "[TRAIN][Iter][300/2700] loss: 34.3619, dice: 0.5066, accu: 0.9900\n",
            "[TRAIN][Iter][305/2700] loss: 73.9321, dice: 0.5011, accu: 0.9866\n",
            "[TRAIN][Iter][310/2700] loss: 31.8389, dice: 0.5123, accu: 0.9852\n",
            "[TRAIN][Iter][315/2700] loss: 4010.9399, dice: 0.5249, accu: 0.9811\n",
            "[TRAIN][Iter][320/2700] loss: 18.5685, dice: 0.5005, accu: 0.9946\n",
            "[TRAIN][Iter][325/2700] loss: 65.7645, dice: 0.5016, accu: 0.9917\n",
            "[TRAIN][Iter][330/2700] loss: 41.3811, dice: 0.5227, accu: 0.9804\n",
            "[TRAIN][Iter][335/2700] loss: 76.8018, dice: 0.4971, accu: 0.9817\n",
            "[TRAIN][Iter][340/2700] loss: 26.8020, dice: 0.5012, accu: 0.9916\n",
            "[TRAIN][Iter][345/2700] loss: 54.6335, dice: 0.4973, accu: 0.9783\n",
            "[TRAIN][Iter][350/2700] loss: 191.4579, dice: 0.5022, accu: 0.9860\n",
            "[TRAIN][Iter][355/2700] loss: 12.3415, dice: 0.4930, accu: 0.9716\n",
            "[TRAIN][Iter][360/2700] loss: 7.5880, dice: 0.4980, accu: 0.9903\n",
            "[TRAIN][Iter][365/2700] loss: 6.6850, dice: 0.4968, accu: 0.9821\n",
            "[TRAIN][Iter][370/2700] loss: 18.4355, dice: 0.5006, accu: 0.9951\n",
            "[TRAIN][Iter][375/2700] loss: 2022.2347, dice: 0.5321, accu: 0.9790\n",
            "[TRAIN][Iter][380/2700] loss: 263.8557, dice: 0.5066, accu: 0.9896\n",
            "[TRAIN][Iter][385/2700] loss: 44.6202, dice: 0.5027, accu: 0.9955\n",
            "[TRAIN][Iter][390/2700] loss: 108.0339, dice: 0.4989, accu: 0.9810\n",
            "[TRAIN][Iter][395/2700] loss: 615.9793, dice: 0.5063, accu: 0.9827\n",
            "[TRAIN][Iter][400/2700] loss: 10.0784, dice: 0.5004, accu: 0.9804\n",
            "[TRAIN][Iter][405/2700] loss: 17.1718, dice: 0.5054, accu: 0.9941\n",
            "[TRAIN][Epoch][3/20] loss: 291.2076, dice: 0.5036, accu: 0.9857\n",
            "Saving model... models/deepvesselnet-fcn-01.model\n",
            "[VAL][Epoch][3/20] loss: 22.9215, dice: 0.4993, accu: 0.9874\n",
            "[TRAIN][Iter][410/2700] loss: 58.4716, dice: 0.5009, accu: 0.9806\n",
            "[TRAIN][Iter][415/2700] loss: 22.4128, dice: 0.4967, accu: 0.9800\n",
            "[TRAIN][Iter][420/2700] loss: 19.8166, dice: 0.4973, accu: 0.9870\n",
            "[TRAIN][Iter][425/2700] loss: 7.9828, dice: 0.4967, accu: 0.9859\n",
            "[TRAIN][Iter][430/2700] loss: 24.5665, dice: 0.5271, accu: 0.9913\n",
            "[TRAIN][Iter][435/2700] loss: 121.6181, dice: 0.5026, accu: 0.9895\n",
            "[TRAIN][Iter][440/2700] loss: 44.5921, dice: 0.4993, accu: 0.9874\n",
            "[TRAIN][Iter][445/2700] loss: 111.3696, dice: 0.4977, accu: 0.9754\n",
            "[TRAIN][Iter][450/2700] loss: 17.6007, dice: 0.5035, accu: 0.9906\n",
            "[TRAIN][Iter][455/2700] loss: 59.8238, dice: 0.4995, accu: 0.9884\n",
            "[TRAIN][Iter][460/2700] loss: 829.2505, dice: 0.5212, accu: 0.9840\n",
            "[TRAIN][Iter][465/2700] loss: 60.3142, dice: 0.5006, accu: 0.9915\n",
            "[TRAIN][Iter][470/2700] loss: 17.7465, dice: 0.5090, accu: 0.9887\n",
            "[TRAIN][Iter][475/2700] loss: 144.1088, dice: 0.5012, accu: 0.9874\n",
            "[TRAIN][Iter][480/2700] loss: 30.2343, dice: 0.5054, accu: 0.9917\n",
            "[TRAIN][Iter][485/2700] loss: 47.1288, dice: 0.5006, accu: 0.9871\n",
            "[TRAIN][Iter][490/2700] loss: 23.9270, dice: 0.5025, accu: 0.9935\n",
            "[TRAIN][Iter][495/2700] loss: 71.4142, dice: 0.5262, accu: 0.9942\n",
            "[TRAIN][Iter][500/2700] loss: 10.9621, dice: 0.4962, accu: 0.9766\n",
            "[TRAIN][Iter][505/2700] loss: 13.5491, dice: 0.4931, accu: 0.9706\n",
            "[TRAIN][Iter][510/2700] loss: 10.3348, dice: 0.5011, accu: 0.9919\n",
            "[TRAIN][Iter][515/2700] loss: 20.7791, dice: 0.4983, accu: 0.9852\n",
            "[TRAIN][Iter][520/2700] loss: 14.4420, dice: 0.5008, accu: 0.9888\n",
            "[TRAIN][Iter][525/2700] loss: 383.4678, dice: 0.5036, accu: 0.9814\n",
            "[TRAIN][Iter][530/2700] loss: 355.0871, dice: 0.5030, accu: 0.9882\n",
            "[TRAIN][Iter][535/2700] loss: 264.4135, dice: 0.5091, accu: 0.9836\n",
            "[TRAIN][Iter][540/2700] loss: 394.8194, dice: 0.5036, accu: 0.9737\n",
            "[TRAIN][Epoch][4/20] loss: 117.7864, dice: 0.5036, accu: 0.9857\n",
            "Saving model... models/deepvesselnet-fcn-01.model\n",
            "[VAL][Epoch][4/20] loss: 22.9215, dice: 0.4993, accu: 0.9874\n",
            "[TRAIN][Iter][545/2700] loss: 59.5885, dice: 0.5036, accu: 0.9912\n",
            "[TRAIN][Iter][550/2700] loss: 102.0971, dice: 0.4984, accu: 0.9817\n",
            "[TRAIN][Iter][555/2700] loss: 16.0414, dice: 0.4987, accu: 0.9897\n",
            "[TRAIN][Iter][560/2700] loss: 226.4835, dice: 0.5015, accu: 0.9718\n",
            "[TRAIN][Iter][565/2700] loss: 31.6145, dice: 0.4971, accu: 0.9807\n",
            "[TRAIN][Iter][570/2700] loss: 37.3541, dice: 0.4947, accu: 0.9725\n",
            "[TRAIN][Iter][575/2700] loss: 464.8057, dice: 0.5033, accu: 0.9727\n",
            "[TRAIN][Iter][580/2700] loss: 5.4713, dice: 0.4975, accu: 0.9886\n",
            "[TRAIN][Iter][585/2700] loss: 49.1282, dice: 0.4981, accu: 0.9793\n",
            "[TRAIN][Iter][590/2700] loss: 12.7878, dice: 0.5002, accu: 0.9910\n",
            "[TRAIN][Iter][595/2700] loss: 783.9357, dice: 0.5255, accu: 0.9855\n",
            "[TRAIN][Iter][600/2700] loss: 15.4598, dice: 0.5060, accu: 0.9906\n",
            "[TRAIN][Iter][605/2700] loss: 80.5950, dice: 0.5015, accu: 0.9881\n",
            "[TRAIN][Iter][610/2700] loss: 80.0397, dice: 0.5152, accu: 0.9875\n",
            "[TRAIN][Iter][615/2700] loss: 16.0792, dice: 0.5081, accu: 0.9887\n",
            "[TRAIN][Iter][620/2700] loss: 45.9694, dice: 0.4976, accu: 0.9792\n",
            "[TRAIN][Iter][625/2700] loss: 10.2077, dice: 0.4986, accu: 0.9908\n",
            "[TRAIN][Iter][630/2700] loss: 493.1178, dice: 0.5060, accu: 0.9866\n",
            "[TRAIN][Iter][635/2700] loss: 33.3006, dice: 0.5009, accu: 0.9933\n",
            "[TRAIN][Iter][640/2700] loss: 214.8186, dice: 0.5260, accu: 0.9923\n",
            "[TRAIN][Iter][645/2700] loss: 220.0324, dice: 0.4991, accu: 0.9789\n",
            "[TRAIN][Iter][650/2700] loss: 9.9576, dice: 0.4988, accu: 0.9894\n",
            "[TRAIN][Iter][655/2700] loss: 40.4590, dice: 0.5033, accu: 0.9924\n",
            "[TRAIN][Iter][660/2700] loss: 211.7547, dice: 0.5083, accu: 0.9900\n",
            "[TRAIN][Iter][665/2700] loss: 29.6529, dice: 0.5003, accu: 0.9933\n",
            "[TRAIN][Iter][670/2700] loss: 185.7009, dice: 0.5055, accu: 0.9810\n",
            "[TRAIN][Iter][675/2700] loss: 188.8701, dice: 0.5032, accu: 0.9870\n",
            "[TRAIN][Epoch][5/20] loss: 135.7527, dice: 0.5036, accu: 0.9857\n",
            "Saving model... models/deepvesselnet-fcn-01.model\n",
            "[VAL][Epoch][5/20] loss: 22.9215, dice: 0.4993, accu: 0.9874\n",
            "[TRAIN][Iter][680/2700] loss: 140.3680, dice: 0.5345, accu: 0.9936\n",
            "[TRAIN][Iter][685/2700] loss: 16.7710, dice: 0.4959, accu: 0.9821\n",
            "[TRAIN][Iter][690/2700] loss: 375.5814, dice: 0.5217, accu: 0.9703\n",
            "[TRAIN][Iter][695/2700] loss: 63.6708, dice: 0.5036, accu: 0.9929\n",
            "[TRAIN][Iter][700/2700] loss: 9.9076, dice: 0.4934, accu: 0.9729\n",
            "[TRAIN][Iter][705/2700] loss: 6.7022, dice: 0.4954, accu: 0.9790\n",
            "[TRAIN][Iter][710/2700] loss: 55.6090, dice: 0.5017, accu: 0.9905\n",
            "[TRAIN][Iter][715/2700] loss: 63.9911, dice: 0.4997, accu: 0.9854\n",
            "[TRAIN][Iter][720/2700] loss: 26.9462, dice: 0.4966, accu: 0.9832\n",
            "[TRAIN][Iter][725/2700] loss: 21.1013, dice: 0.4975, accu: 0.9851\n",
            "[TRAIN][Iter][730/2700] loss: 8.2755, dice: 0.5035, accu: 0.9883\n",
            "[TRAIN][Iter][735/2700] loss: 167.2731, dice: 0.4982, accu: 0.9779\n",
            "[TRAIN][Iter][740/2700] loss: 66.4730, dice: 0.5064, accu: 0.9883\n",
            "[TRAIN][Iter][745/2700] loss: 207.8203, dice: 0.5169, accu: 0.9774\n",
            "[TRAIN][Iter][750/2700] loss: 83.2846, dice: 0.5068, accu: 0.9861\n",
            "[TRAIN][Iter][755/2700] loss: 36.6879, dice: 0.4995, accu: 0.9882\n",
            "[TRAIN][Iter][760/2700] loss: 41.4749, dice: 0.5028, accu: 0.9912\n",
            "[TRAIN][Iter][765/2700] loss: 49.5749, dice: 0.5003, accu: 0.9911\n",
            "[TRAIN][Iter][770/2700] loss: 6.9850, dice: 0.4989, accu: 0.9818\n",
            "[TRAIN][Iter][775/2700] loss: 42.5603, dice: 0.5024, accu: 0.9881\n",
            "[TRAIN][Iter][780/2700] loss: 84.9122, dice: 0.5015, accu: 0.9843\n",
            "[TRAIN][Iter][785/2700] loss: 81.6288, dice: 0.5023, accu: 0.9926\n",
            "[TRAIN][Iter][790/2700] loss: 25.4009, dice: 0.5114, accu: 0.9949\n",
            "[TRAIN][Iter][795/2700] loss: 101.1667, dice: 0.5012, accu: 0.9915\n",
            "[TRAIN][Iter][800/2700] loss: 33.6220, dice: 0.4987, accu: 0.9834\n",
            "[TRAIN][Iter][805/2700] loss: 18.4819, dice: 0.4991, accu: 0.9897\n",
            "[TRAIN][Iter][810/2700] loss: 712.1091, dice: 0.5071, accu: 0.9844\n",
            "[TRAIN][Epoch][6/20] loss: 94.3844, dice: 0.5036, accu: 0.9857\n",
            "Saving model... models/deepvesselnet-fcn-01.model\n",
            "[VAL][Epoch][6/20] loss: 22.9215, dice: 0.4993, accu: 0.9874\n",
            "[TRAIN][Iter][815/2700] loss: 27.8855, dice: 0.5038, accu: 0.9868\n",
            "[TRAIN][Iter][820/2700] loss: 382.7767, dice: 0.5043, accu: 0.9877\n",
            "[TRAIN][Iter][825/2700] loss: 24.9768, dice: 0.4941, accu: 0.9657\n",
            "[TRAIN][Iter][830/2700] loss: 37.5232, dice: 0.5030, accu: 0.9896\n",
            "[TRAIN][Iter][835/2700] loss: 1496.7305, dice: 0.5255, accu: 0.9896\n",
            "[TRAIN][Iter][840/2700] loss: 306.2719, dice: 0.5146, accu: 0.9904\n",
            "[TRAIN][Iter][845/2700] loss: 30.3080, dice: 0.4996, accu: 0.9871\n",
            "[TRAIN][Iter][850/2700] loss: 82.0669, dice: 0.4963, accu: 0.9779\n",
            "[TRAIN][Iter][855/2700] loss: 137.4860, dice: 0.5264, accu: 0.9873\n",
            "[TRAIN][Iter][860/2700] loss: 8.0924, dice: 0.4996, accu: 0.9937\n",
            "[TRAIN][Iter][865/2700] loss: 17.1200, dice: 0.5159, accu: 0.9824\n",
            "[TRAIN][Iter][870/2700] loss: 42.1463, dice: 0.5013, accu: 0.9862\n",
            "[TRAIN][Iter][875/2700] loss: 18.0369, dice: 0.5060, accu: 0.9935\n",
            "[TRAIN][Iter][880/2700] loss: 510.3466, dice: 0.5077, accu: 0.9862\n",
            "[TRAIN][Iter][885/2700] loss: 12.2905, dice: 0.4967, accu: 0.9834\n",
            "[TRAIN][Iter][890/2700] loss: 431.5584, dice: 0.5126, accu: 0.9926\n",
            "[TRAIN][Iter][895/2700] loss: 27.0470, dice: 0.5021, accu: 0.9934\n",
            "[TRAIN][Iter][900/2700] loss: 99.3464, dice: 0.4983, accu: 0.9839\n",
            "[TRAIN][Iter][905/2700] loss: 8.2577, dice: 0.5004, accu: 0.9940\n",
            "[TRAIN][Iter][910/2700] loss: 10.3581, dice: 0.5011, accu: 0.9899\n",
            "[TRAIN][Iter][915/2700] loss: 11.3823, dice: 0.4950, accu: 0.9778\n",
            "[TRAIN][Iter][920/2700] loss: 99.8843, dice: 0.4973, accu: 0.9753\n",
            "[TRAIN][Iter][925/2700] loss: 101.4089, dice: 0.4969, accu: 0.9786\n",
            "[TRAIN][Iter][930/2700] loss: 40.8317, dice: 0.4959, accu: 0.9781\n",
            "[TRAIN][Iter][935/2700] loss: 18.0977, dice: 0.4955, accu: 0.9804\n",
            "[TRAIN][Iter][940/2700] loss: 58.0902, dice: 0.5042, accu: 0.9937\n",
            "[TRAIN][Iter][945/2700] loss: 17.1085, dice: 0.5026, accu: 0.9886\n",
            "[TRAIN][Epoch][7/20] loss: 150.2752, dice: 0.5036, accu: 0.9857\n",
            "Saving model... models/deepvesselnet-fcn-01.model\n",
            "[VAL][Epoch][7/20] loss: 22.9215, dice: 0.4993, accu: 0.9874\n",
            "[TRAIN][Iter][950/2700] loss: 763.5109, dice: 0.5221, accu: 0.9892\n",
            "[TRAIN][Iter][955/2700] loss: 15.0596, dice: 0.4946, accu: 0.9737\n",
            "[TRAIN][Iter][960/2700] loss: 12.6322, dice: 0.4967, accu: 0.9851\n",
            "[TRAIN][Iter][965/2700] loss: 12.0338, dice: 0.4951, accu: 0.9787\n",
            "[TRAIN][Iter][970/2700] loss: 12.5305, dice: 0.4996, accu: 0.9932\n",
            "[TRAIN][Iter][975/2700] loss: 99.0821, dice: 0.5157, accu: 0.9953\n",
            "[TRAIN][Iter][980/2700] loss: 34.3958, dice: 0.4978, accu: 0.9855\n",
            "[TRAIN][Iter][985/2700] loss: 194.9299, dice: 0.5069, accu: 0.9833\n",
            "[TRAIN][Iter][990/2700] loss: 47.8666, dice: 0.5170, accu: 0.9889\n",
            "[TRAIN][Iter][995/2700] loss: 107.0814, dice: 0.5220, accu: 0.9829\n",
            "[TRAIN][Iter][1000/2700] loss: 17.7138, dice: 0.4962, accu: 0.9827\n",
            "[TRAIN][Iter][1005/2700] loss: 54.4387, dice: 0.5008, accu: 0.9802\n",
            "[TRAIN][Iter][1010/2700] loss: 93.9688, dice: 0.5016, accu: 0.9904\n",
            "[TRAIN][Iter][1015/2700] loss: 142.2443, dice: 0.5080, accu: 0.9862\n",
            "[TRAIN][Iter][1020/2700] loss: 7.5753, dice: 0.4947, accu: 0.9776\n",
            "[TRAIN][Iter][1025/2700] loss: 20.7649, dice: 0.4969, accu: 0.9848\n",
            "[TRAIN][Iter][1030/2700] loss: 10.1255, dice: 0.4983, accu: 0.9905\n",
            "[TRAIN][Iter][1035/2700] loss: 67.7581, dice: 0.5008, accu: 0.9880\n",
            "[TRAIN][Iter][1040/2700] loss: 16.3223, dice: 0.4990, accu: 0.9895\n",
            "[TRAIN][Iter][1045/2700] loss: 28.3605, dice: 0.5071, accu: 0.9889\n",
            "[TRAIN][Iter][1050/2700] loss: 63.6735, dice: 0.5242, accu: 0.9911\n",
            "[TRAIN][Iter][1055/2700] loss: 12.1486, dice: 0.5035, accu: 0.9889\n",
            "[TRAIN][Iter][1060/2700] loss: 15.0412, dice: 0.5061, accu: 0.9944\n",
            "[TRAIN][Iter][1065/2700] loss: 140.5424, dice: 0.4952, accu: 0.9738\n",
            "[TRAIN][Iter][1070/2700] loss: 205.2664, dice: 0.5012, accu: 0.9851\n",
            "[TRAIN][Iter][1075/2700] loss: 9.3921, dice: 0.4957, accu: 0.9787\n",
            "[TRAIN][Iter][1080/2700] loss: 10.2949, dice: 0.4985, accu: 0.9835\n",
            "[TRAIN][Epoch][8/20] loss: 82.0279, dice: 0.5035, accu: 0.9855\n",
            "Saving model... models/deepvesselnet-fcn-01.model\n",
            "[VAL][Epoch][8/20] loss: 22.9215, dice: 0.4993, accu: 0.9874\n",
            "[TRAIN][Iter][1085/2700] loss: 23.5103, dice: 0.5151, accu: 0.9911\n",
            "[TRAIN][Iter][1090/2700] loss: 382.6430, dice: 0.5304, accu: 0.9869\n",
            "[TRAIN][Iter][1095/2700] loss: 8.9345, dice: 0.4972, accu: 0.9859\n",
            "[TRAIN][Iter][1100/2700] loss: 84.1711, dice: 0.4962, accu: 0.9776\n",
            "[TRAIN][Iter][1105/2700] loss: 19.3358, dice: 0.4977, accu: 0.9867\n",
            "[TRAIN][Iter][1110/2700] loss: 6.6388, dice: 0.5007, accu: 0.9928\n",
            "[TRAIN][Iter][1115/2700] loss: 31.7195, dice: 0.4955, accu: 0.9799\n",
            "[TRAIN][Iter][1120/2700] loss: 770.9385, dice: 0.5183, accu: 0.9722\n",
            "[TRAIN][Iter][1125/2700] loss: 19.3789, dice: 0.5044, accu: 0.9959\n",
            "[TRAIN][Iter][1130/2700] loss: 169.1399, dice: 0.5041, accu: 0.9901\n",
            "[TRAIN][Iter][1135/2700] loss: 16.5234, dice: 0.4959, accu: 0.9799\n",
            "[TRAIN][Iter][1140/2700] loss: 16.3590, dice: 0.5007, accu: 0.9943\n",
            "[TRAIN][Iter][1145/2700] loss: 10.4428, dice: 0.4964, accu: 0.9847\n",
            "[TRAIN][Iter][1150/2700] loss: 21.0723, dice: 0.5043, accu: 0.9926\n",
            "[TRAIN][Iter][1155/2700] loss: 8.2291, dice: 0.4959, accu: 0.9823\n",
            "[TRAIN][Iter][1160/2700] loss: 23.6151, dice: 0.5021, accu: 0.9946\n",
            "[TRAIN][Iter][1165/2700] loss: 12.7997, dice: 0.4951, accu: 0.9769\n",
            "[TRAIN][Iter][1170/2700] loss: 128.2282, dice: 0.4999, accu: 0.9811\n",
            "[TRAIN][Iter][1175/2700] loss: 756.7492, dice: 0.5167, accu: 0.9869\n",
            "[TRAIN][Iter][1180/2700] loss: 93.8497, dice: 0.4999, accu: 0.9839\n",
            "[TRAIN][Iter][1185/2700] loss: 18.1175, dice: 0.5021, accu: 0.9892\n",
            "[TRAIN][Iter][1190/2700] loss: 20.6215, dice: 0.4975, accu: 0.9838\n",
            "[TRAIN][Iter][1195/2700] loss: 404.7433, dice: 0.5050, accu: 0.9787\n",
            "[TRAIN][Iter][1200/2700] loss: 7.8256, dice: 0.5012, accu: 0.9905\n",
            "[TRAIN][Iter][1205/2700] loss: 39.8992, dice: 0.4985, accu: 0.9795\n",
            "[TRAIN][Iter][1210/2700] loss: 90.5631, dice: 0.5014, accu: 0.9914\n",
            "[TRAIN][Iter][1215/2700] loss: 33.8202, dice: 0.5233, accu: 0.9803\n",
            "[TRAIN][Epoch][9/20] loss: 119.2544, dice: 0.5035, accu: 0.9855\n",
            "Saving model... models/deepvesselnet-fcn-01.model\n",
            "[VAL][Epoch][9/20] loss: 22.9215, dice: 0.4993, accu: 0.9874\n",
            "[TRAIN][Iter][1220/2700] loss: 32.2166, dice: 0.5053, accu: 0.9904\n",
            "[TRAIN][Iter][1225/2700] loss: 90.9485, dice: 0.5024, accu: 0.9903\n",
            "[TRAIN][Iter][1230/2700] loss: 8.7093, dice: 0.5208, accu: 0.9815\n",
            "[TRAIN][Iter][1235/2700] loss: 35.5554, dice: 0.4969, accu: 0.9829\n",
            "[TRAIN][Iter][1240/2700] loss: 13.7817, dice: 0.4989, accu: 0.9918\n",
            "[TRAIN][Iter][1245/2700] loss: 9.4101, dice: 0.4988, accu: 0.9928\n",
            "[TRAIN][Iter][1250/2700] loss: 121.6801, dice: 0.5085, accu: 0.9932\n",
            "[TRAIN][Iter][1255/2700] loss: 8.4408, dice: 0.4969, accu: 0.9808\n",
            "[TRAIN][Iter][1260/2700] loss: 65.1166, dice: 0.4950, accu: 0.9737\n",
            "[TRAIN][Iter][1265/2700] loss: 30.8741, dice: 0.4980, accu: 0.9854\n",
            "[TRAIN][Iter][1270/2700] loss: 10.6570, dice: 0.4976, accu: 0.9851\n",
            "[TRAIN][Iter][1275/2700] loss: 45.8905, dice: 0.5001, accu: 0.9864\n",
            "[TRAIN][Iter][1280/2700] loss: 12.5699, dice: 0.4985, accu: 0.9874\n",
            "[TRAIN][Iter][1285/2700] loss: 335.3800, dice: 0.5073, accu: 0.9852\n",
            "[TRAIN][Iter][1290/2700] loss: 21.0391, dice: 0.5218, accu: 0.9839\n",
            "[TRAIN][Iter][1295/2700] loss: 236.1079, dice: 0.5089, accu: 0.9934\n",
            "[TRAIN][Iter][1300/2700] loss: 165.1793, dice: 0.5057, accu: 0.9863\n",
            "[TRAIN][Iter][1305/2700] loss: 55.0713, dice: 0.4997, accu: 0.9844\n",
            "[TRAIN][Iter][1310/2700] loss: 39.7000, dice: 0.4977, accu: 0.9809\n",
            "[TRAIN][Iter][1315/2700] loss: 13.1404, dice: 0.4979, accu: 0.9864\n",
            "[TRAIN][Iter][1320/2700] loss: 85.6531, dice: 0.5036, accu: 0.9847\n",
            "[TRAIN][Iter][1325/2700] loss: 29.9651, dice: 0.4996, accu: 0.9812\n",
            "[TRAIN][Iter][1330/2700] loss: 18.7273, dice: 0.4991, accu: 0.9872\n",
            "[TRAIN][Iter][1335/2700] loss: 110.5699, dice: 0.5261, accu: 0.9905\n",
            "[TRAIN][Iter][1340/2700] loss: 27.2989, dice: 0.4960, accu: 0.9782\n",
            "[TRAIN][Iter][1345/2700] loss: 16.3574, dice: 0.5022, accu: 0.9891\n",
            "[TRAIN][Iter][1350/2700] loss: 27.5819, dice: 0.5135, accu: 0.9812\n",
            "[TRAIN][Epoch][10/20] loss: 61.7638, dice: 0.5036, accu: 0.9857\n",
            "Saving model... models/deepvesselnet-fcn-01.model\n",
            "[VAL][Epoch][10/20] loss: 22.9215, dice: 0.4993, accu: 0.9874\n",
            "[TRAIN][Iter][1355/2700] loss: 10.9421, dice: 0.5268, accu: 0.9927\n",
            "[TRAIN][Iter][1360/2700] loss: 14.8868, dice: 0.4967, accu: 0.9846\n",
            "[TRAIN][Iter][1365/2700] loss: 2665.7507, dice: 0.5226, accu: 0.9891\n",
            "[TRAIN][Iter][1370/2700] loss: 13.1597, dice: 0.4949, accu: 0.9785\n",
            "[TRAIN][Iter][1375/2700] loss: 13.7226, dice: 0.4940, accu: 0.9740\n",
            "[TRAIN][Iter][1380/2700] loss: 6.9623, dice: 0.4998, accu: 0.9818\n",
            "[TRAIN][Iter][1385/2700] loss: 11.9769, dice: 0.4969, accu: 0.9842\n",
            "[TRAIN][Iter][1390/2700] loss: 12.1365, dice: 0.5115, accu: 0.9957\n",
            "[TRAIN][Iter][1395/2700] loss: 10.1595, dice: 0.4984, accu: 0.9894\n",
            "[TRAIN][Iter][1400/2700] loss: 12.5550, dice: 0.4980, accu: 0.9806\n",
            "[TRAIN][Iter][1405/2700] loss: 16.9791, dice: 0.5033, accu: 0.9921\n",
            "[TRAIN][Iter][1410/2700] loss: 31.4519, dice: 0.5056, accu: 0.9944\n",
            "[TRAIN][Iter][1415/2700] loss: 36.6162, dice: 0.4988, accu: 0.9833\n",
            "[TRAIN][Iter][1420/2700] loss: 906.9277, dice: 0.5218, accu: 0.9851\n",
            "[TRAIN][Iter][1425/2700] loss: 61.6163, dice: 0.4995, accu: 0.9738\n",
            "[TRAIN][Iter][1430/2700] loss: 350.6564, dice: 0.5091, accu: 0.9880\n",
            "[TRAIN][Iter][1435/2700] loss: 78.2577, dice: 0.5076, accu: 0.9905\n",
            "[TRAIN][Iter][1440/2700] loss: 74.3079, dice: 0.5011, accu: 0.9884\n",
            "[TRAIN][Iter][1445/2700] loss: 16.1906, dice: 0.4948, accu: 0.9767\n",
            "[TRAIN][Iter][1450/2700] loss: 28.5981, dice: 0.4984, accu: 0.9887\n",
            "[TRAIN][Iter][1455/2700] loss: 103.5150, dice: 0.5041, accu: 0.9912\n",
            "[TRAIN][Iter][1460/2700] loss: 16.3673, dice: 0.4970, accu: 0.9845\n",
            "[TRAIN][Iter][1465/2700] loss: 341.8932, dice: 0.5058, accu: 0.9814\n",
            "[TRAIN][Iter][1470/2700] loss: 25.3628, dice: 0.5015, accu: 0.9898\n",
            "[TRAIN][Iter][1475/2700] loss: 165.4619, dice: 0.5104, accu: 0.9739\n",
            "[TRAIN][Iter][1480/2700] loss: 34.7824, dice: 0.4999, accu: 0.9913\n",
            "[TRAIN][Iter][1485/2700] loss: 9.4082, dice: 0.4975, accu: 0.9877\n",
            "[TRAIN][Epoch][11/20] loss: 187.8017, dice: 0.5036, accu: 0.9856\n",
            "Saving model... models/deepvesselnet-fcn-01.model\n",
            "[VAL][Epoch][11/20] loss: 22.9215, dice: 0.4993, accu: 0.9874\n",
            "[TRAIN][Iter][1490/2700] loss: 74.6788, dice: 0.5284, accu: 0.9948\n",
            "[TRAIN][Iter][1495/2700] loss: 41.6350, dice: 0.5066, accu: 0.9897\n",
            "[TRAIN][Iter][1500/2700] loss: 108.5231, dice: 0.5007, accu: 0.9879\n",
            "[TRAIN][Iter][1505/2700] loss: 6.0320, dice: 0.4951, accu: 0.9774\n",
            "[TRAIN][Iter][1510/2700] loss: 1113.7271, dice: 0.5013, accu: 0.9716\n",
            "[TRAIN][Iter][1515/2700] loss: 7.3628, dice: 0.4982, accu: 0.9903\n",
            "[TRAIN][Iter][1520/2700] loss: 1998.1234, dice: 0.5347, accu: 0.9858\n",
            "[TRAIN][Iter][1525/2700] loss: 71.0001, dice: 0.4980, accu: 0.9823\n",
            "[TRAIN][Iter][1530/2700] loss: 20.6557, dice: 0.4972, accu: 0.9797\n",
            "[TRAIN][Iter][1535/2700] loss: 13.4681, dice: 0.4963, accu: 0.9840\n",
            "[TRAIN][Iter][1540/2700] loss: 14.6598, dice: 0.4995, accu: 0.9930\n",
            "[TRAIN][Iter][1545/2700] loss: 588.5511, dice: 0.5143, accu: 0.9834\n",
            "[TRAIN][Iter][1550/2700] loss: 17.9231, dice: 0.4939, accu: 0.9747\n",
            "[TRAIN][Iter][1555/2700] loss: 14.9156, dice: 0.4984, accu: 0.9862\n",
            "[TRAIN][Iter][1560/2700] loss: 459.5502, dice: 0.5120, accu: 0.9907\n",
            "[TRAIN][Iter][1565/2700] loss: 119.1808, dice: 0.5017, accu: 0.9923\n",
            "[TRAIN][Iter][1570/2700] loss: 20.5947, dice: 0.4979, accu: 0.9844\n",
            "[TRAIN][Iter][1575/2700] loss: 43.4263, dice: 0.5016, accu: 0.9925\n",
            "[TRAIN][Iter][1580/2700] loss: 56.8334, dice: 0.4987, accu: 0.9856\n",
            "[TRAIN][Iter][1585/2700] loss: 98.8788, dice: 0.4981, accu: 0.9757\n",
            "[TRAIN][Iter][1590/2700] loss: 602.2346, dice: 0.5118, accu: 0.9896\n",
            "[TRAIN][Iter][1595/2700] loss: 8.4343, dice: 0.4971, accu: 0.9855\n",
            "[TRAIN][Iter][1600/2700] loss: 31.0414, dice: 0.5050, accu: 0.9898\n",
            "[TRAIN][Iter][1605/2700] loss: 159.3058, dice: 0.4997, accu: 0.9773\n",
            "[TRAIN][Iter][1610/2700] loss: 26.6752, dice: 0.5007, accu: 0.9899\n",
            "[TRAIN][Iter][1615/2700] loss: 32.4548, dice: 0.5009, accu: 0.9912\n",
            "[TRAIN][Iter][1620/2700] loss: 217.8407, dice: 0.5085, accu: 0.9871\n",
            "[TRAIN][Epoch][12/20] loss: 221.0262, dice: 0.5036, accu: 0.9856\n",
            "Saving model... models/deepvesselnet-fcn-01.model\n",
            "[VAL][Epoch][12/20] loss: 22.9215, dice: 0.4993, accu: 0.9874\n",
            "[TRAIN][Iter][1625/2700] loss: 79.6922, dice: 0.5035, accu: 0.9894\n",
            "[TRAIN][Iter][1630/2700] loss: 17.8960, dice: 0.5001, accu: 0.9753\n",
            "[TRAIN][Iter][1635/2700] loss: 29.0405, dice: 0.5227, accu: 0.9815\n",
            "[TRAIN][Iter][1640/2700] loss: 6.3736, dice: 0.4973, accu: 0.9857\n",
            "[TRAIN][Iter][1645/2700] loss: 1031.7146, dice: 0.5260, accu: 0.9913\n",
            "[TRAIN][Iter][1650/2700] loss: 717.6156, dice: 0.5152, accu: 0.9834\n",
            "[TRAIN][Iter][1655/2700] loss: 167.8440, dice: 0.4956, accu: 0.9752\n",
            "[TRAIN][Iter][1660/2700] loss: 16.8623, dice: 0.5050, accu: 0.9939\n",
            "[TRAIN][Iter][1665/2700] loss: 36.2336, dice: 0.5001, accu: 0.9887\n",
            "[TRAIN][Iter][1670/2700] loss: 41.4055, dice: 0.5011, accu: 0.9948\n",
            "[TRAIN][Iter][1675/2700] loss: 156.3938, dice: 0.5023, accu: 0.9774\n",
            "[TRAIN][Iter][1680/2700] loss: 7.8037, dice: 0.4977, accu: 0.9832\n",
            "[TRAIN][Iter][1685/2700] loss: 26.5142, dice: 0.4988, accu: 0.9878\n",
            "[TRAIN][Iter][1690/2700] loss: 42.5808, dice: 0.4988, accu: 0.9823\n",
            "[TRAIN][Iter][1695/2700] loss: 45.2124, dice: 0.5017, accu: 0.9785\n",
            "[TRAIN][Iter][1700/2700] loss: 39.3786, dice: 0.4946, accu: 0.9769\n",
            "[TRAIN][Iter][1705/2700] loss: 82.9405, dice: 0.4996, accu: 0.9877\n",
            "[TRAIN][Iter][1710/2700] loss: 10.9467, dice: 0.4984, accu: 0.9870\n",
            "[TRAIN][Iter][1715/2700] loss: 7.1941, dice: 0.4993, accu: 0.9863\n",
            "[TRAIN][Iter][1720/2700] loss: 9.8671, dice: 0.4988, accu: 0.9922\n",
            "[TRAIN][Iter][1725/2700] loss: 78.2706, dice: 0.4979, accu: 0.9814\n",
            "[TRAIN][Iter][1730/2700] loss: 5.0361, dice: 0.4981, accu: 0.9836\n",
            "[TRAIN][Iter][1735/2700] loss: 132.2756, dice: 0.5135, accu: 0.9897\n",
            "[TRAIN][Iter][1740/2700] loss: 44.9528, dice: 0.5085, accu: 0.9903\n",
            "[TRAIN][Iter][1745/2700] loss: 306.7679, dice: 0.5049, accu: 0.9839\n",
            "[TRAIN][Iter][1750/2700] loss: 17.1183, dice: 0.5048, accu: 0.9964\n",
            "[TRAIN][Iter][1755/2700] loss: 214.6555, dice: 0.5128, accu: 0.9906\n",
            "[TRAIN][Epoch][13/20] loss: 124.9106, dice: 0.5036, accu: 0.9857\n",
            "Saving model... models/deepvesselnet-fcn-01.model\n",
            "[VAL][Epoch][13/20] loss: 22.9215, dice: 0.4993, accu: 0.9874\n",
            "[TRAIN][Iter][1760/2700] loss: 255.7671, dice: 0.5079, accu: 0.9928\n",
            "[TRAIN][Iter][1765/2700] loss: 124.0917, dice: 0.5213, accu: 0.9816\n",
            "[TRAIN][Iter][1770/2700] loss: 7.9508, dice: 0.4986, accu: 0.9856\n",
            "[TRAIN][Iter][1775/2700] loss: 54.1131, dice: 0.5035, accu: 0.9903\n",
            "[TRAIN][Iter][1780/2700] loss: 19.3193, dice: 0.5015, accu: 0.9818\n",
            "[TRAIN][Iter][1785/2700] loss: 1479.5989, dice: 0.5373, accu: 0.9676\n",
            "[TRAIN][Iter][1790/2700] loss: 21.5066, dice: 0.5026, accu: 0.9896\n",
            "[TRAIN][Iter][1795/2700] loss: 8.0864, dice: 0.4998, accu: 0.9911\n",
            "[TRAIN][Iter][1800/2700] loss: 241.5118, dice: 0.5061, accu: 0.9835\n",
            "[TRAIN][Iter][1805/2700] loss: 358.1703, dice: 0.5002, accu: 0.9828\n",
            "[TRAIN][Iter][1810/2700] loss: 147.2034, dice: 0.4986, accu: 0.9763\n",
            "[TRAIN][Iter][1815/2700] loss: 116.7035, dice: 0.5124, accu: 0.9901\n",
            "[TRAIN][Iter][1820/2700] loss: 23.0815, dice: 0.5005, accu: 0.9888\n",
            "[TRAIN][Iter][1825/2700] loss: 11.8972, dice: 0.5004, accu: 0.9942\n",
            "[TRAIN][Iter][1830/2700] loss: 146.8029, dice: 0.5008, accu: 0.9827\n",
            "[TRAIN][Iter][1835/2700] loss: 22.3481, dice: 0.5002, accu: 0.9903\n",
            "[TRAIN][Iter][1840/2700] loss: 7.0178, dice: 0.4985, accu: 0.9838\n",
            "[TRAIN][Iter][1845/2700] loss: 49.7898, dice: 0.4979, accu: 0.9773\n",
            "[TRAIN][Iter][1850/2700] loss: 6.9443, dice: 0.4989, accu: 0.9901\n",
            "[TRAIN][Iter][1855/2700] loss: 588.8214, dice: 0.5054, accu: 0.9796\n",
            "[TRAIN][Iter][1860/2700] loss: 14.1916, dice: 0.4974, accu: 0.9863\n",
            "[TRAIN][Iter][1865/2700] loss: 82.8635, dice: 0.5053, accu: 0.9865\n",
            "[TRAIN][Iter][1870/2700] loss: 39.6569, dice: 0.5060, accu: 0.9941\n",
            "[TRAIN][Iter][1875/2700] loss: 70.0560, dice: 0.4999, accu: 0.9870\n",
            "[TRAIN][Iter][1880/2700] loss: 10.8621, dice: 0.4982, accu: 0.9889\n",
            "[TRAIN][Iter][1885/2700] loss: 55.9295, dice: 0.5003, accu: 0.9851\n",
            "[TRAIN][Iter][1890/2700] loss: 23.7561, dice: 0.4970, accu: 0.9852\n",
            "[TRAIN][Epoch][14/20] loss: 147.7052, dice: 0.5036, accu: 0.9857\n",
            "Saving model... models/deepvesselnet-fcn-01.model\n",
            "[VAL][Epoch][14/20] loss: 22.9215, dice: 0.4993, accu: 0.9874\n",
            "[TRAIN][Iter][1895/2700] loss: 58.9139, dice: 0.4970, accu: 0.9776\n",
            "[TRAIN][Iter][1900/2700] loss: 430.4425, dice: 0.5244, accu: 0.9883\n",
            "[TRAIN][Iter][1905/2700] loss: 11.8026, dice: 0.5017, accu: 0.9948\n",
            "[TRAIN][Iter][1910/2700] loss: 367.7536, dice: 0.5068, accu: 0.9880\n",
            "[TRAIN][Iter][1915/2700] loss: 9.1604, dice: 0.4975, accu: 0.9879\n",
            "[TRAIN][Iter][1920/2700] loss: 19.9421, dice: 0.5079, accu: 0.9885\n",
            "[TRAIN][Iter][1925/2700] loss: 212.1988, dice: 0.5287, accu: 0.9953\n",
            "[TRAIN][Iter][1930/2700] loss: 19.6475, dice: 0.4975, accu: 0.9876\n",
            "[TRAIN][Iter][1935/2700] loss: 83.2590, dice: 0.4913, accu: 0.9619\n",
            "[TRAIN][Iter][1940/2700] loss: 33.8417, dice: 0.5023, accu: 0.9931\n",
            "[TRAIN][Iter][1945/2700] loss: 5.4221, dice: 0.4989, accu: 0.9913\n",
            "[TRAIN][Iter][1950/2700] loss: 13.3376, dice: 0.4947, accu: 0.9757\n",
            "[TRAIN][Iter][1955/2700] loss: 20.9274, dice: 0.4966, accu: 0.9822\n",
            "[TRAIN][Iter][1960/2700] loss: 170.8783, dice: 0.5216, accu: 0.9819\n",
            "[TRAIN][Iter][1965/2700] loss: 20.4735, dice: 0.5010, accu: 0.9922\n",
            "[TRAIN][Iter][1970/2700] loss: 24.5868, dice: 0.5097, accu: 0.9906\n",
            "[TRAIN][Iter][1975/2700] loss: 29.4978, dice: 0.5011, accu: 0.9887\n",
            "[TRAIN][Iter][1980/2700] loss: 8.8816, dice: 0.4982, accu: 0.9910\n",
            "[TRAIN][Iter][1985/2700] loss: 42.6458, dice: 0.4970, accu: 0.9827\n",
            "[TRAIN][Iter][1990/2700] loss: 78.0562, dice: 0.5015, accu: 0.9826\n",
            "[TRAIN][Iter][1995/2700] loss: 41.4513, dice: 0.4989, accu: 0.9874\n",
            "[TRAIN][Iter][2000/2700] loss: 63.0046, dice: 0.5283, accu: 0.9936\n",
            "[TRAIN][Iter][2005/2700] loss: 22.2246, dice: 0.4926, accu: 0.9698\n",
            "[TRAIN][Iter][2010/2700] loss: 40.1411, dice: 0.4984, accu: 0.9851\n",
            "[TRAIN][Iter][2015/2700] loss: 117.6865, dice: 0.5008, accu: 0.9801\n",
            "[TRAIN][Iter][2020/2700] loss: 6.8694, dice: 0.4959, accu: 0.9805\n",
            "[TRAIN][Iter][2025/2700] loss: 9.7721, dice: 0.5068, accu: 0.9959\n",
            "[TRAIN][Epoch][15/20] loss: 72.6970, dice: 0.5036, accu: 0.9857\n",
            "Saving model... models/deepvesselnet-fcn-01.model\n",
            "[VAL][Epoch][15/20] loss: 22.9215, dice: 0.4993, accu: 0.9874\n",
            "[TRAIN][Iter][2030/2700] loss: 90.2245, dice: 0.4984, accu: 0.9817\n",
            "[TRAIN][Iter][2035/2700] loss: 14.4309, dice: 0.4973, accu: 0.9830\n",
            "[TRAIN][Iter][2040/2700] loss: 159.6975, dice: 0.5020, accu: 0.9874\n",
            "[TRAIN][Iter][2045/2700] loss: 20.3593, dice: 0.4984, accu: 0.9882\n",
            "[TRAIN][Iter][2050/2700] loss: 17.4370, dice: 0.4970, accu: 0.9843\n",
            "[TRAIN][Iter][2055/2700] loss: 13.3385, dice: 0.5161, accu: 0.9924\n",
            "[TRAIN][Iter][2060/2700] loss: 198.3243, dice: 0.5039, accu: 0.9890\n",
            "[TRAIN][Iter][2065/2700] loss: 21.0537, dice: 0.4996, accu: 0.9850\n",
            "[TRAIN][Iter][2070/2700] loss: 46.0473, dice: 0.5136, accu: 0.9901\n",
            "[TRAIN][Iter][2075/2700] loss: 42.7849, dice: 0.4990, accu: 0.9876\n",
            "[TRAIN][Iter][2080/2700] loss: 6.9144, dice: 0.4973, accu: 0.9869\n",
            "[TRAIN][Iter][2085/2700] loss: 46.9183, dice: 0.4993, accu: 0.9826\n",
            "[TRAIN][Iter][2090/2700] loss: 55.3451, dice: 0.4974, accu: 0.9841\n",
            "[TRAIN][Iter][2095/2700] loss: 11.5095, dice: 0.4987, accu: 0.9825\n",
            "[TRAIN][Iter][2100/2700] loss: 80.9788, dice: 0.4958, accu: 0.9794\n",
            "[TRAIN][Iter][2105/2700] loss: 77.3592, dice: 0.4961, accu: 0.9741\n",
            "[TRAIN][Iter][2110/2700] loss: 94.4048, dice: 0.5054, accu: 0.9855\n",
            "[TRAIN][Iter][2115/2700] loss: 44.0476, dice: 0.4984, accu: 0.9884\n",
            "[TRAIN][Iter][2120/2700] loss: 600.4824, dice: 0.5325, accu: 0.9871\n",
            "[TRAIN][Iter][2125/2700] loss: 6397.7363, dice: 0.5277, accu: 0.9692\n",
            "[TRAIN][Iter][2130/2700] loss: 152.6877, dice: 0.5073, accu: 0.9938\n",
            "[TRAIN][Iter][2135/2700] loss: 25.6351, dice: 0.5048, accu: 0.9964\n",
            "[TRAIN][Iter][2140/2700] loss: 36.5866, dice: 0.5008, accu: 0.9831\n",
            "[TRAIN][Iter][2145/2700] loss: 45.2739, dice: 0.5074, accu: 0.9938\n",
            "[TRAIN][Iter][2150/2700] loss: 94.5310, dice: 0.5000, accu: 0.9880\n",
            "[TRAIN][Iter][2155/2700] loss: 125.2135, dice: 0.5036, accu: 0.9801\n",
            "[TRAIN][Iter][2160/2700] loss: 8.8179, dice: 0.4989, accu: 0.9902\n",
            "[TRAIN][Epoch][16/20] loss: 315.8570, dice: 0.5036, accu: 0.9857\n",
            "Saving model... models/deepvesselnet-fcn-01.model\n",
            "[VAL][Epoch][16/20] loss: 22.9215, dice: 0.4993, accu: 0.9874\n",
            "[TRAIN][Iter][2165/2700] loss: 11.1626, dice: 0.5006, accu: 0.9906\n",
            "[TRAIN][Iter][2170/2700] loss: 49.2458, dice: 0.5031, accu: 0.9895\n",
            "[TRAIN][Iter][2175/2700] loss: 7.2879, dice: 0.4970, accu: 0.9849\n",
            "[TRAIN][Iter][2180/2700] loss: 14.2228, dice: 0.4955, accu: 0.9781\n",
            "[TRAIN][Iter][2185/2700] loss: 30.7519, dice: 0.5097, accu: 0.9937\n",
            "[TRAIN][Iter][2190/2700] loss: 58.5305, dice: 0.4970, accu: 0.9758\n",
            "[TRAIN][Iter][2195/2700] loss: 466.4684, dice: 0.5081, accu: 0.9852\n",
            "[TRAIN][Iter][2200/2700] loss: 52.6548, dice: 0.5004, accu: 0.9920\n",
            "[TRAIN][Iter][2205/2700] loss: 92.6081, dice: 0.5242, accu: 0.9903\n",
            "[TRAIN][Iter][2210/2700] loss: 10.6152, dice: 0.4992, accu: 0.9887\n",
            "[TRAIN][Iter][2215/2700] loss: 706.6686, dice: 0.5284, accu: 0.9941\n",
            "[TRAIN][Iter][2220/2700] loss: 64.5218, dice: 0.4977, accu: 0.9782\n",
            "[TRAIN][Iter][2225/2700] loss: 15.2404, dice: 0.4986, accu: 0.9830\n",
            "[TRAIN][Iter][2230/2700] loss: 191.2386, dice: 0.4999, accu: 0.9773\n",
            "[TRAIN][Iter][2235/2700] loss: 28.2336, dice: 0.4983, accu: 0.9838\n",
            "[TRAIN][Iter][2240/2700] loss: 62.7335, dice: 0.5127, accu: 0.9944\n",
            "[TRAIN][Iter][2245/2700] loss: 35.1460, dice: 0.4975, accu: 0.9801\n",
            "[TRAIN][Iter][2250/2700] loss: 23.1254, dice: 0.4980, accu: 0.9878\n",
            "[TRAIN][Iter][2255/2700] loss: 11.6252, dice: 0.5003, accu: 0.9890\n",
            "[TRAIN][Iter][2260/2700] loss: 422.6428, dice: 0.5301, accu: 0.9967\n",
            "[TRAIN][Iter][2265/2700] loss: 159.8139, dice: 0.5005, accu: 0.9797\n",
            "[TRAIN][Iter][2270/2700] loss: 426.8333, dice: 0.5081, accu: 0.9902\n",
            "[TRAIN][Iter][2275/2700] loss: 23.2542, dice: 0.4909, accu: 0.9631\n",
            "[TRAIN][Iter][2280/2700] loss: 124.0902, dice: 0.5017, accu: 0.9844\n",
            "[TRAIN][Iter][2285/2700] loss: 80.0836, dice: 0.5008, accu: 0.9900\n",
            "[TRAIN][Iter][2290/2700] loss: 70.2412, dice: 0.5013, accu: 0.9874\n",
            "[TRAIN][Iter][2295/2700] loss: 10.8148, dice: 0.4964, accu: 0.9836\n",
            "[TRAIN][Epoch][17/20] loss: 120.3650, dice: 0.5036, accu: 0.9856\n",
            "Saving model... models/deepvesselnet-fcn-01.model\n",
            "[VAL][Epoch][17/20] loss: 22.9215, dice: 0.4993, accu: 0.9874\n",
            "[TRAIN][Iter][2300/2700] loss: 19.2569, dice: 0.4994, accu: 0.9818\n",
            "[TRAIN][Iter][2305/2700] loss: 45.5048, dice: 0.4946, accu: 0.9748\n",
            "[TRAIN][Iter][2310/2700] loss: 93.3830, dice: 0.5067, accu: 0.9819\n",
            "[TRAIN][Iter][2315/2700] loss: 168.6649, dice: 0.5033, accu: 0.9885\n",
            "[TRAIN][Iter][2320/2700] loss: 45.7797, dice: 0.5006, accu: 0.9891\n",
            "[TRAIN][Iter][2325/2700] loss: 17.3856, dice: 0.4997, accu: 0.9873\n",
            "[TRAIN][Iter][2330/2700] loss: 16.7485, dice: 0.5018, accu: 0.9887\n",
            "[TRAIN][Iter][2335/2700] loss: 62.4959, dice: 0.4990, accu: 0.9876\n",
            "[TRAIN][Iter][2340/2700] loss: 23.0850, dice: 0.5005, accu: 0.9942\n",
            "[TRAIN][Iter][2345/2700] loss: 155.6719, dice: 0.5034, accu: 0.9847\n",
            "[TRAIN][Iter][2350/2700] loss: 38.5677, dice: 0.5010, accu: 0.9910\n",
            "[TRAIN][Iter][2355/2700] loss: 507.8552, dice: 0.5077, accu: 0.9887\n",
            "[TRAIN][Iter][2360/2700] loss: 78.1096, dice: 0.5028, accu: 0.9853\n",
            "[TRAIN][Iter][2365/2700] loss: 11.8241, dice: 0.5077, accu: 0.9833\n",
            "[TRAIN][Iter][2370/2700] loss: 332.4189, dice: 0.5282, accu: 0.9953\n",
            "[TRAIN][Iter][2375/2700] loss: 10.2711, dice: 0.4999, accu: 0.9893\n",
            "[TRAIN][Iter][2380/2700] loss: 16.2414, dice: 0.4982, accu: 0.9839\n",
            "[TRAIN][Iter][2385/2700] loss: 11.7666, dice: 0.4952, accu: 0.9796\n",
            "[TRAIN][Iter][2390/2700] loss: 11.7074, dice: 0.4961, accu: 0.9809\n",
            "[TRAIN][Iter][2395/2700] loss: 26.8447, dice: 0.5034, accu: 0.9768\n",
            "[TRAIN][Iter][2400/2700] loss: 223.3218, dice: 0.5144, accu: 0.9890\n",
            "[TRAIN][Iter][2405/2700] loss: 44.6319, dice: 0.5008, accu: 0.9869\n",
            "[TRAIN][Iter][2410/2700] loss: 300.7260, dice: 0.5135, accu: 0.9806\n",
            "[TRAIN][Iter][2415/2700] loss: 93.2750, dice: 0.5255, accu: 0.9925\n",
            "[TRAIN][Iter][2420/2700] loss: 60.1247, dice: 0.5031, accu: 0.9925\n",
            "[TRAIN][Iter][2425/2700] loss: 10.2705, dice: 0.4953, accu: 0.9799\n",
            "[TRAIN][Iter][2430/2700] loss: 5.9117, dice: 0.4953, accu: 0.9800\n",
            "[TRAIN][Epoch][18/20] loss: 90.0683, dice: 0.5036, accu: 0.9857\n",
            "Saving model... models/deepvesselnet-fcn-01.model\n",
            "[VAL][Epoch][18/20] loss: 22.9215, dice: 0.4993, accu: 0.9874\n",
            "[TRAIN][Iter][2435/2700] loss: 10.6616, dice: 0.4970, accu: 0.9860\n",
            "[TRAIN][Iter][2440/2700] loss: 34.6899, dice: 0.5033, accu: 0.9893\n",
            "[TRAIN][Iter][2445/2700] loss: 114.8896, dice: 0.5099, accu: 0.9925\n",
            "[TRAIN][Iter][2450/2700] loss: 3320.5222, dice: 0.5219, accu: 0.9843\n",
            "[TRAIN][Iter][2455/2700] loss: 124.5990, dice: 0.5021, accu: 0.9805\n",
            "[TRAIN][Iter][2460/2700] loss: 27.9339, dice: 0.5048, accu: 0.9958\n",
            "[TRAIN][Iter][2465/2700] loss: 898.0212, dice: 0.5143, accu: 0.9714\n",
            "[TRAIN][Iter][2470/2700] loss: 245.2494, dice: 0.5311, accu: 0.9900\n",
            "[TRAIN][Iter][2475/2700] loss: 33.7054, dice: 0.4987, accu: 0.9789\n",
            "[TRAIN][Iter][2480/2700] loss: 19.7379, dice: 0.5080, accu: 0.9866\n",
            "[TRAIN][Iter][2485/2700] loss: 13.4878, dice: 0.4970, accu: 0.9853\n",
            "[TRAIN][Iter][2490/2700] loss: 145.6033, dice: 0.5018, accu: 0.9922\n",
            "[TRAIN][Iter][2495/2700] loss: 465.9590, dice: 0.5141, accu: 0.9869\n",
            "[TRAIN][Iter][2500/2700] loss: 102.7598, dice: 0.5024, accu: 0.9904\n",
            "[TRAIN][Iter][2505/2700] loss: 321.7840, dice: 0.5063, accu: 0.9849\n",
            "[TRAIN][Iter][2510/2700] loss: 8.9995, dice: 0.4985, accu: 0.9906\n",
            "[TRAIN][Iter][2515/2700] loss: 13.0928, dice: 0.4960, accu: 0.9822\n",
            "[TRAIN][Iter][2520/2700] loss: 57.3651, dice: 0.5001, accu: 0.9785\n",
            "[TRAIN][Iter][2525/2700] loss: 11.0476, dice: 0.4958, accu: 0.9767\n",
            "[TRAIN][Iter][2530/2700] loss: 76.3950, dice: 0.4987, accu: 0.9762\n",
            "[TRAIN][Iter][2535/2700] loss: 11.5531, dice: 0.4987, accu: 0.9865\n",
            "[TRAIN][Iter][2540/2700] loss: 6.7695, dice: 0.5000, accu: 0.9946\n",
            "[TRAIN][Iter][2545/2700] loss: 97.0287, dice: 0.5003, accu: 0.9867\n",
            "[TRAIN][Iter][2550/2700] loss: 12.4062, dice: 0.4984, accu: 0.9873\n",
            "[TRAIN][Iter][2555/2700] loss: 11.7949, dice: 0.4953, accu: 0.9770\n",
            "[TRAIN][Iter][2560/2700] loss: 12.3478, dice: 0.4994, accu: 0.9897\n",
            "[TRAIN][Iter][2565/2700] loss: 10.3482, dice: 0.5030, accu: 0.9932\n",
            "[TRAIN][Epoch][19/20] loss: 229.9538, dice: 0.5036, accu: 0.9857\n",
            "Saving model... models/deepvesselnet-fcn-01.model\n",
            "[VAL][Epoch][19/20] loss: 22.9215, dice: 0.4993, accu: 0.9874\n",
            "[TRAIN][Iter][2570/2700] loss: 119.9629, dice: 0.4972, accu: 0.9794\n",
            "[TRAIN][Iter][2575/2700] loss: 65.4905, dice: 0.5027, accu: 0.9941\n",
            "[TRAIN][Iter][2580/2700] loss: 11.6154, dice: 0.4987, accu: 0.9891\n",
            "[TRAIN][Iter][2585/2700] loss: 6.1481, dice: 0.4987, accu: 0.9907\n",
            "[TRAIN][Iter][2590/2700] loss: 78.3094, dice: 0.5002, accu: 0.9885\n",
            "[TRAIN][Iter][2595/2700] loss: 1331.9841, dice: 0.5229, accu: 0.9822\n",
            "[TRAIN][Iter][2600/2700] loss: 6.0981, dice: 0.4961, accu: 0.9822\n",
            "[TRAIN][Iter][2605/2700] loss: 318.9125, dice: 0.5261, accu: 0.9813\n",
            "[TRAIN][Iter][2610/2700] loss: 8.3984, dice: 0.4958, accu: 0.9816\n",
            "[TRAIN][Iter][2615/2700] loss: 176.3216, dice: 0.5016, accu: 0.9840\n",
            "[TRAIN][Iter][2620/2700] loss: 36.1659, dice: 0.4995, accu: 0.9832\n",
            "[TRAIN][Iter][2625/2700] loss: 5.7052, dice: 0.4985, accu: 0.9891\n",
            "[TRAIN][Iter][2630/2700] loss: 323.9431, dice: 0.5132, accu: 0.9846\n",
            "[TRAIN][Iter][2635/2700] loss: 124.4226, dice: 0.5292, accu: 0.9946\n",
            "[TRAIN][Iter][2640/2700] loss: 162.1155, dice: 0.5158, accu: 0.9921\n",
            "[TRAIN][Iter][2645/2700] loss: 16.0494, dice: 0.4983, accu: 0.9878\n",
            "[TRAIN][Iter][2650/2700] loss: 6.4695, dice: 0.4961, accu: 0.9838\n",
            "[TRAIN][Iter][2655/2700] loss: 33.8441, dice: 0.5037, accu: 0.9951\n",
            "[TRAIN][Iter][2660/2700] loss: 32.6771, dice: 0.4948, accu: 0.9764\n",
            "[TRAIN][Iter][2665/2700] loss: 92.0059, dice: 0.5000, accu: 0.9840\n",
            "[TRAIN][Iter][2670/2700] loss: 7.0325, dice: 0.4968, accu: 0.9864\n",
            "[TRAIN][Iter][2675/2700] loss: 101.1549, dice: 0.5027, accu: 0.9828\n",
            "[TRAIN][Iter][2680/2700] loss: 121.2512, dice: 0.5034, accu: 0.9863\n",
            "[TRAIN][Iter][2685/2700] loss: 26.8253, dice: 0.5027, accu: 0.9808\n",
            "[TRAIN][Iter][2690/2700] loss: 31.7635, dice: 0.5095, accu: 0.9889\n",
            "[TRAIN][Iter][2695/2700] loss: 10.3718, dice: 0.4952, accu: 0.9798\n",
            "[TRAIN][Iter][2700/2700] loss: 24.7217, dice: 0.4968, accu: 0.9823\n",
            "[TRAIN][Epoch][20/20] loss: 121.4726, dice: 0.5036, accu: 0.9856\n",
            "Saving model... models/deepvesselnet-fcn-01.model\n",
            "[VAL][Epoch][20/20] loss: 22.9215, dice: 0.4993, accu: 0.9874\n",
            "FINISH\n",
            "Saving model... models/deepvesselnet-fcn-01.model\n"
          ]
        }
      ],
      "source": [
        "# model = torch.load('models/deepvesselnet-02.model').to(device)\n",
        "solver.train(model, train_loader, val_loader, log_nth=5, num_epochs=total_epochs, model_save_path=model_save_path)\n",
        "model.save(model_save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RZBf47JcrDI"
      },
      "source": [
        "# Save output as nifty format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from matplotlib.lines import Line2D\n",
        "from torch.autograd import Variable\n",
        "from itertools import product\n",
        "\n",
        "def patchify(volume, patch_size, step):\n",
        "    \"\"\"\n",
        "\n",
        "    :param volume:\n",
        "    :param patch_size:\n",
        "    :param step:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    assert len(volume.shape) == 4\n",
        "\n",
        "    _, v_h, v_w, v_d = volume.shape\n",
        "\n",
        "    s_h, s_w, s_d = step\n",
        "\n",
        "    _, p_h, p_w, p_d = patch_size\n",
        "\n",
        "    # Calculate the number of patch in each axis\n",
        "    n_w = np.ceil(1.0*(v_w-p_w)/s_w+1)\n",
        "    n_h = np.ceil(1.0*(v_h-p_h)/s_h+1)\n",
        "    n_d = np.ceil(1.0*(v_d-p_d)/s_d+1)\n",
        "\n",
        "    n_w = int(n_w)\n",
        "    n_h = int(n_h)\n",
        "    n_d = int(n_d)\n",
        "\n",
        "    pad_w = (n_w - 1) * s_w + p_w - v_w\n",
        "    pad_h = (n_h - 1) * s_h + p_h - v_h\n",
        "    pad_d = (n_d - 1) * s_d + p_d - v_d\n",
        "    # print(volume.shape, (0, pad_h, 0, pad_w, 0, pad_d))\n",
        "    volume = F.pad(volume, (0, pad_d, 0, pad_w, 0, pad_h), 'constant')\n",
        "    # print(volume.shape)\n",
        "    patches = torch.zeros((n_h, n_w, n_d,)+patch_size, dtype=volume.dtype)\n",
        "\n",
        "    for i, j, k in product(range(n_h), range(n_w), range(n_d)):\n",
        "        patches[i, j, k] = volume[:, (i * s_h):(i * s_h) + p_h, (j * s_w):(j * s_w) + p_w, (k * s_d):(k * s_d) + p_d]\n",
        "\n",
        "    return patches\n",
        "\n",
        "\n",
        "def unpatchify(patches, step, imsize, scale_factor):\n",
        "    \"\"\"\n",
        "\n",
        "    :param patches:\n",
        "    :param step:\n",
        "    :param imsize:\n",
        "    :param scale_factor:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    assert len(patches.shape) == 7\n",
        "\n",
        "    c, r_h, r_w, r_d = imsize\n",
        "    s_h, s_w, s_d = tuple(scale_factor*np.array(step))\n",
        "\n",
        "    n_h, n_w, n_d, _, p_h, p_w, p_d = patches.shape\n",
        "\n",
        "    v_w = (n_w - 1) * s_w + p_w\n",
        "    v_h = (n_h - 1) * s_h + p_h\n",
        "    v_d = (n_d - 1) * s_d + p_d\n",
        "\n",
        "    volume = torch.zeros((c, v_h, v_w, v_d), dtype=patches.dtype)\n",
        "    divisor = torch.zeros((c, v_h, v_w, v_d), dtype=patches.dtype)\n",
        "#     print(volume.shape, imsize)\n",
        "\n",
        "    for i, j, k in product(range(n_h), range(n_w), range(n_d)):\n",
        "        patch = patches[i, j, k]\n",
        "        volume[:, (i * s_h):(i * s_h) + p_h, (j * s_w):(j * s_w) + p_w, (k * s_d):(k * s_d) + p_d] += patch\n",
        "        divisor[:, (i * s_h):(i * s_h) + p_h, (j * s_w):(j * s_w) + p_w, (k * s_d):(k * s_d) + p_d] += 1\n",
        "    volume /= divisor\n",
        "    return volume[:, 0:r_h, 0:r_w, 0:r_d]\n",
        "\n",
        "def test(model, volume, patch_size=64, stride=60, device=torch.cpu):\n",
        "    model.eval()\n",
        "\n",
        "    patch_size = (1, patch_size, patch_size, patch_size)\n",
        "    stride = (stride, stride, stride)\n",
        "\n",
        "    patches = patchify(volume, patch_size, stride)\n",
        "    patch_shape = patches.shape\n",
        "    patches = patches.view((-1,) + patch_size)\n",
        "    patches = patches.cuda().type(torch.cuda.FloatTensor) if device.type == 'cuda' else patches.type(torch.FloatTensor)\n",
        "\n",
        "    output = torch.zeros((0, ) + patch_size[1:]).type(torch.FloatTensor)\n",
        "\n",
        "    batch_size = 5 # user input\n",
        "    num = int(np.ceil(1.0 * patches.shape[0] / batch_size))\n",
        "\n",
        "    for i in range(num):\n",
        "        model_output = model.forward(patches[batch_size*i:batch_size*i + batch_size])\n",
        "\n",
        "        _, preds = torch.max(model_output, 1)\n",
        "        preds = preds.type(torch.FloatTensor)\n",
        "        # preds = preds.cuda().type(torch.cuda.FloatTensor) if device.type == 'cuda' else preds.cpu().type(torch.FloatTensor)\n",
        "\n",
        "        output = torch.cat((output, preds), 0)\n",
        "\n",
        "    new_shape = patch_shape\n",
        "    output = unpatchify(output.view(new_shape), stride, volume.shape, 1)\n",
        "    output = output.squeeze(0)\n",
        "    \n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c7mQ1GHjcrDJ",
        "outputId": "7df0e365-8631-4ab4-8efe-4808d0d07db1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SyntheticData2] [raw_dir_name] dataset\\train\\images\n",
            "[SyntheticData2] [seg_dir_name] dataset\\train\\labels\n"
          ]
        },
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 67.0 MiB for an array with shape (292, 292, 206) and data type int32",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\User\\Downloads\\deepvesselnet (2)\\selffinal.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Downloads/deepvesselnet%20%282%29/selffinal.ipynb#ch0000091?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mmodels/deepvesselnet-fcn2-01.model\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Downloads/deepvesselnet%20%282%29/selffinal.ipynb#ch0000091?line=2'>3</a>\u001b[0m \u001b[39m# create training dataset\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/User/Downloads/deepvesselnet%20%282%29/selffinal.ipynb#ch0000091?line=3'>4</a>\u001b[0m test_synthetic \u001b[39m=\u001b[39m SyntheticData2(root_path\u001b[39m=\u001b[39;49mtraining_set_path, patch_size\u001b[39m=\u001b[39;49mpatch_size, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Downloads/deepvesselnet%20%282%29/selffinal.ipynb#ch0000091?line=4'>5</a>\u001b[0m \u001b[39m# index of input image\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Downloads/deepvesselnet%20%282%29/selffinal.ipynb#ch0000091?line=5'>6</a>\u001b[0m idx \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
            "\u001b[1;32mc:\\Users\\User\\Downloads\\deepvesselnet (2)\\selffinal.ipynb Cell 34\u001b[0m in \u001b[0;36mSyntheticData2.__init__\u001b[1;34m(self, root_path, patch_size, mode)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Downloads/deepvesselnet%20%282%29/selffinal.ipynb#ch0000091?line=30'>31</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatches_seg \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Downloads/deepvesselnet%20%282%29/selffinal.ipynb#ch0000091?line=31'>32</a>\u001b[0m \u001b[39mfor\u001b[39;00m image_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_names:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Downloads/deepvesselnet%20%282%29/selffinal.ipynb#ch0000091?line=32'>33</a>\u001b[0m     raw, seg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_patches_from_nii(image_name)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Downloads/deepvesselnet%20%282%29/selffinal.ipynb#ch0000091?line=33'>34</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Downloads/deepvesselnet%20%282%29/selffinal.ipynb#ch0000091?line=34'>35</a>\u001b[0m         \u001b[39m# print(f'[{self.mode}]', image_name, raw.shape, seg.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Downloads/deepvesselnet%20%282%29/selffinal.ipynb#ch0000091?line=35'>36</a>\u001b[0m         raw, seg \u001b[39m=\u001b[39m raw\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), seg\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n",
            "\u001b[1;32mc:\\Users\\User\\Downloads\\deepvesselnet (2)\\selffinal.ipynb Cell 34\u001b[0m in \u001b[0;36mSyntheticData2.get_patches_from_nii\u001b[1;34m(self, image_name)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Downloads/deepvesselnet%20%282%29/selffinal.ipynb#ch0000091?line=72'>73</a>\u001b[0m seg_proxy \u001b[39m=\u001b[39m nib\u001b[39m.\u001b[39mload(seg_img_name)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Downloads/deepvesselnet%20%282%29/selffinal.ipynb#ch0000091?line=74'>75</a>\u001b[0m \u001b[39m# Get dataobj of proxy\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Downloads/deepvesselnet%20%282%29/selffinal.ipynb#ch0000091?line=75'>76</a>\u001b[0m raw_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(raw_proxy\u001b[39m.\u001b[39;49mdataobj)\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39;49mint32)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Downloads/deepvesselnet%20%282%29/selffinal.ipynb#ch0000091?line=76'>77</a>\u001b[0m seg_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(seg_proxy\u001b[39m.\u001b[39mdataobj)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint32)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Downloads/deepvesselnet%20%282%29/selffinal.ipynb#ch0000091?line=78'>79</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m:\n",
            "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 67.0 MiB for an array with shape (292, 292, 206) and data type int32"
          ]
        }
      ],
      "source": [
        "model = torch.load('models/deepvesselnet-fcn2-01.model').to(device)\n",
        "\n",
        "# create training dataset\n",
        "test_synthetic = SyntheticData2(root_path=training_set_path, patch_size=patch_size, mode='test')\n",
        "# index of input image\n",
        "idx = 1\n",
        "# get actual volume and its segmentation mask\n",
        "volume, segmentation = test_synthetic[idx]\n",
        "print('volume:', volume.shape)\n",
        "# get input image and segmentation image paths\n",
        "raw_path, seg_path = test_synthetic.get_image_path(idx)\n",
        "print('raw_path:', raw_path)\n",
        "print('seg_path:', seg_path)\n",
        "# get affine transformation matrix of image loaded above\n",
        "raw_affine = nib.load(raw_path).affine\n",
        "print('raw_affine:\\n', raw_affine)\n",
        "\n",
        "# get model output on loaded image\n",
        "output = test(model, volume, device=device)\n",
        "print('[output]', output.shape)\n",
        "dice = dice_coeff(output, segmentation, pred=True).detach().cpu().numpy()\n",
        "print(\"Dice coefficient of output: \", dice)\n",
        "print(\"Num seg pixels: \", np.argwhere(segmentation.detach().cpu().numpy() == 1).size)\n",
        "print(\"Num output pixels: \", np.argwhere(output.detach().cpu().numpy() == 1).size)\n",
        "\n",
        "# save output in local disk space\n",
        "save_path = os.path.join('saved_output', os.path.split(raw_path)[-1])\n",
        "print('save_path:', save_path)\n",
        "out_img = nib.Nifti1Image(output.detach().cpu().numpy(), raw_affine)\n",
        "nib.save(out_img, save_path)\n",
        "print('output saved at:', save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Szw0MaikEWpz"
      },
      "outputs": [],
      "source": [
        "# plot raw image\n",
        "raw_widget = nw.NiftiWidget(raw_path)\n",
        "raw_widget.nifti_plotter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTOogUnuEYP5"
      },
      "outputs": [],
      "source": [
        "# plot segmentation image\n",
        "seg_widget = nw.NiftiWidget(seg_path)\n",
        "seg_widget.nifti_plotter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cue5tSSDE55"
      },
      "outputs": [],
      "source": [
        "# plot output image\n",
        "test_widget = nw.NiftiWidget(save_path)\n",
        "test_widget.nifti_plotter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb3W68eUUCAA"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "HJV0Ricrf2IU",
        "VJbzW_NG0dvE",
        "BbjFph5SdCAa",
        "e9uXQNvlX4CU",
        "7RX0TIwD9HGh",
        "PjC8XGvXNaR0"
      ],
      "name": "fiverr_rehash001_pytorch-02.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
